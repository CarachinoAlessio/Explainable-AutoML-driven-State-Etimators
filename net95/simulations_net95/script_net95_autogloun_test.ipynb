{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T11:20:27.208228Z",
     "start_time": "2024-05-18T11:20:26.123421Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8003b9514dd6505b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:39:13.619610400Z",
     "start_time": "2024-02-23T15:39:13.585967500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net95/net_95_v1/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net95/net_95_v1/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "815b96bd2fa485ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:39:15.440864200Z",
     "start_time": "2024-02-23T15:39:15.429397500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_input = 206\n",
    "num_output = 95\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d1e1333565ba3f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.75 GB / 21.50 GB (82.5%)\n",
      "Disk Space Avail:   875.13 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       206\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    18151.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.19s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.95s of the 3.95s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.79s of the 3.79s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.94799e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1758. Best iteration is:\n",
      "\t[1752]\tvalid_set's l2: 2.93572e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.26s of the -0.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.6s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.49 GB / 21.50 GB (81.4%)\n",
      "Disk Space Avail:   875.08 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       207\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17932.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.93425e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1821. Best iteration is:\n",
      "\t[1453]\tvalid_set's l2: 2.91573e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.51 GB / 21.50 GB (81.4%)\n",
      "Disk Space Avail:   875.04 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       208\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17948.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91337e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.36s of the 0.36s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 57. Best iteration is:\n",
      "\t[57]\tvalid_set's l2: 7.1181e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.933, 'LightGBM': 0.067}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.49 GB / 21.50 GB (81.3%)\n",
      "Disk Space Avail:   875.00 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       209\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17928.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.38s of the 1.38s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.01386e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 349. Best iteration is:\n",
      "\t[349]\tvalid_set's l2: 3.3518e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.51 GB / 21.50 GB (81.4%)\n",
      "Disk Space Avail:   874.95 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       210\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17951.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.85s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85603e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.50 GB / 21.50 GB (81.4%)\n",
      "Disk Space Avail:   874.91 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       211\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17944.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.93s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.76s of the 3.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97221e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.07s of the 1.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 241. Best iteration is:\n",
      "\t[236]\tvalid_set's l2: 3.32655e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.07s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.684, 'LightGBM': 0.316}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.50 GB / 21.50 GB (81.4%)\n",
      "Disk Space Avail:   874.87 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       212\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17944.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.07s of the 1.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 173. Best iteration is:\n",
      "\t[166]\tvalid_set's l2: 3.09059e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.17s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.45 GB / 21.50 GB (81.2%)\n",
      "Disk Space Avail:   874.82 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       213\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17870.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.84s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.0s of the 4.0s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.85s of the 3.84s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89051e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1745. Best iteration is:\n",
      "\t[1700]\tvalid_set's l2: 2.86008e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.16s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.45 GB / 21.50 GB (81.2%)\n",
      "Disk Space Avail:   874.78 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       214\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17874.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87994e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.14s of the 1.13s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 291. Best iteration is:\n",
      "\t[161]\tvalid_set's l2: 2.77596e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.55, 'LightGBMXT': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.45 GB / 21.50 GB (81.2%)\n",
      "Disk Space Avail:   874.73 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       215\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17870.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87418e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1882. Best iteration is:\n",
      "\t[1461]\tvalid_set's l2: 2.85422e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.04s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.46 GB / 21.50 GB (81.2%)\n",
      "Disk Space Avail:   874.69 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       216\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17874.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.47s of the 1.46s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 402. Best iteration is:\n",
      "\t[165]\tvalid_set's l2: 2.73498e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.6, 'LightGBMXT': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.42 GB / 21.50 GB (81.0%)\n",
      "Disk Space Avail:   874.65 GB / 1006.85 GB (86.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       217\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17860.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.89s of the 1.89s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.09s of the 0.09s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.95s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -40.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.522, 'LightGBM': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.08 GB / 21.50 GB (79.4%)\n",
      "Disk Space Avail:   874.30 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       218\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17489.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.02s of the 2.02s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.35s of the 0.34s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.59s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -40.53s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.57s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.95 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       219\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17464.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.97s of the 1.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.34s of the 0.34s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t42.06s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -41.99s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.60 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       220\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17466.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.0s of the 4.0s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.85s of the 3.85s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85178e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.39s of the 0.39s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 61. Best iteration is:\n",
      "\t[61]\tvalid_set's l2: 5.1754e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.85, 'LightGBM': 0.15}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.56 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       221\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17469.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.80252e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.06s of the 1.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 245. Best iteration is:\n",
      "\t[219]\tvalid_set's l2: 2.87883e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.538, 'LightGBM': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.51 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       222\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17468.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.80902e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.93s of the 0.93s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 201. Best iteration is:\n",
      "\t[191]\tvalid_set's l2: 2.9412e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.562, 'LightGBM': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.3%)\n",
      "Disk Space Avail:   873.47 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       223\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17472.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.41s of the 1.41s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.8238e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 362. Best iteration is:\n",
      "\t[265]\tvalid_set's l2: 2.88491e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.529, 'LightGBM': 0.471}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_17\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_18\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.43 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       224\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17467.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.84s of the 1.84s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 537. Best iteration is:\n",
      "\t[364]\tvalid_set's l2: 2.89587e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_18\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_19\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.38 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       225\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17466.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.83s of the 1.83s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 517. Best iteration is:\n",
      "\t[286]\tvalid_set's l2: 2.91469e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_19\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_20\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.34 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       226\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17468.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.54s of the 1.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 407. Best iteration is:\n",
      "\t[234]\tvalid_set's l2: 2.97467e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.588, 'LightGBM': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_20\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_21\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.30 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       227\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17468.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.21s of the 2.2s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 692. Best iteration is:\n",
      "\t[510]\tvalid_set's l2: 2.97832e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_21\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_22\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.26 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       228\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17463.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.95s of the 3.95s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.95033e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.39s of the 0.39s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 64. Best iteration is:\n",
      "\t[64]\tvalid_set's l2: 5.02957e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.81, 'LightGBM': 0.19}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_22\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_23\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.21 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       229\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17470.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9617e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1827. Best iteration is:\n",
      "\t[1708]\tvalid_set's l2: 2.93991e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_23\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_24\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.17 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       230\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17469.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.26s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.95s of the 3.95s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83929e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.21s of the 1.21s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 307. Best iteration is:\n",
      "\t[302]\tvalid_set's l2: 3.00672e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.26s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_24\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_25\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.12 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       231\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17467.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.0156e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.9s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_25\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_26\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.08 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       232\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17468.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.96107e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.19s of the 1.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 301. Best iteration is:\n",
      "\t[299]\tvalid_set's l2: 3.10385e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_26\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_27\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.04 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   873.04 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       233\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17469.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97115e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1762. Best iteration is:\n",
      "\t[1666]\tvalid_set's l2: 2.94815e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_27\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_28\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   872.99 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       234\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17464.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.07s of the 2.07s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.6s of the 0.6s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.46s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -40.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.556, 'LightGBMXT': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_28\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_29\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   872.64 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       235\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17460.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.91s of the 1.91s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.36s of the 0.36s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -40.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.571, 'LightGBMXT': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_29\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_30\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   872.29 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       236\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17455.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.8988e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.99s of the 0.99s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 239. Best iteration is:\n",
      "\t[133]\tvalid_set's l2: 2.56342e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.68, 'LightGBMXT': 0.32}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_30\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_31\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       17.03 GB / 21.50 GB (79.2%)\n",
      "Disk Space Avail:   872.25 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       237\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17460.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.15s of the 2.14s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.71s of the 0.71s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.11s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -38.68s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.692, 'LightGBMXT': 0.308}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_31\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_32\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.77 GB / 21.50 GB (78.0%)\n",
      "Disk Space Avail:   871.90 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       238\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17196.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.26s of the 2.25s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.85s of the 0.84s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t38.14s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -37.59s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.5, 'LightGBMXT': 0.417, 'RandomForestMSE': 0.083}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.64s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_32\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_33\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.78 GB / 21.50 GB (78.0%)\n",
      "Disk Space Avail:   871.55 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       239\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17203.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.72634e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1929. Best iteration is:\n",
      "\t[1547]\tvalid_set's l2: 2.71938e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_33\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_34\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.78 GB / 21.50 GB (78.1%)\n",
      "Disk Space Avail:   871.50 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       240\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17207.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.72s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.97s of the 3.96s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.85s of the 1.85s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.41s of the 0.41s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.28s of the -39.41s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.438, 'LightGBM': 0.438, 'RandomForestMSE': 0.125}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.46s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_34\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_35\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.80 GB / 21.50 GB (78.1%)\n",
      "Disk Space Avail:   871.15 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       241\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17221.09 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.31s of the 2.3s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.94s of the 0.94s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.55s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -38.91s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.5, 'LightGBMXT': 0.389, 'RandomForestMSE': 0.111}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_35\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_36\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.81 GB / 21.50 GB (78.2%)\n",
      "Disk Space Avail:   870.81 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       242\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17234.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.16s of the 2.16s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.72s of the 0.72s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.9s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -39.47s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.529, 'LightGBMXT': 0.412, 'RandomForestMSE': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_36\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_37\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.81 GB / 21.50 GB (78.2%)\n",
      "Disk Space Avail:   870.46 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       243\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17231.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.46s of the 1.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91775e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.06s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.588, 'LightGBMXT': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_37\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_38\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.81 GB / 21.50 GB (78.2%)\n",
      "Disk Space Avail:   870.41 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       244\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17235.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.49s of the 1.49s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.05s of the 0.05s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.59s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -41.85s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.48, 'LightGBM': 0.4, 'RandomForestMSE': 0.12}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.9s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_38\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_39\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.77 GB / 21.50 GB (78.0%)\n",
      "Disk Space Avail:   870.06 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       245\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17195.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.9s of the 1.9s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.25s of the 0.25s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t38.16s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -38.21s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.444, 'LightGBM': 0.444, 'RandomForestMSE': 0.111}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_39\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_40\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.86 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   869.71 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       246\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17280.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.63s of the 1.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.84109e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 482. Best iteration is:\n",
      "\t[311]\tvalid_set's l2: 2.88233e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.522, 'LightGBM': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_40\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_41\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.86 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   869.67 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       247\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17286.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.90751e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.53s of the 0.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 105. Best iteration is:\n",
      "\t[104]\tvalid_set's l2: 3.01831e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_41\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_42\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.63 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       248\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.88613e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.24s of the 1.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 352. Best iteration is:\n",
      "\t[342]\tvalid_set's l2: 2.94182e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.524, 'LightGBM': 0.476}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_42\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_43\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.58 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       249\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.1s of the 2.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 658. Best iteration is:\n",
      "\t[564]\tvalid_set's l2: 2.93578e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_43\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_44\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.54 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       250\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87694e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.19s of the 1.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 326. Best iteration is:\n",
      "\t[302]\tvalid_set's l2: 2.93908e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.526, 'LightGBM': 0.474}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_44\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_45\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.50 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       251\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.90789e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.66s of the 0.65s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 136. Best iteration is:\n",
      "\t[135]\tvalid_set's l2: 3.013e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_45\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_46\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.45 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87363e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.18s of the 0.18s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 14. Best iteration is:\n",
      "\t[14]\tvalid_set's l2: 0.000216807\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_46\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_47\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.41 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       253\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.24s of the 2.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 707. Best iteration is:\n",
      "\t[554]\tvalid_set's l2: 2.93199e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_47\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_48\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   869.37 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       254\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17292.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.4s of the 2.4s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.22s of the 0.22s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.4s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -39.5s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.52, 'LightGBM': 0.44, 'RandomForestMSE': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_48\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_49\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   869.02 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       255\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17299.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.05948e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.15s of the 1.15s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 317. Best iteration is:\n",
      "\t[246]\tvalid_set's l2: 3.01111e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.52, 'LightGBMXT': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_49\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_50\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.97 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       256\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9451e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.44s of the 0.44s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 75. Best iteration is:\n",
      "\t[75]\tvalid_set's l2: 3.49651e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_50\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_51\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.93 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       257\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85235e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.21s of the 1.21s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 331. Best iteration is:\n",
      "\t[244]\tvalid_set's l2: 2.99017e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.56, 'LightGBM': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_51\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_52\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.88 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       258\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.94717e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.63s of the 0.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 131. Best iteration is:\n",
      "\t[129]\tvalid_set's l2: 3.05243e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_52\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_53\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.84 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       259\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.07s of the 2.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 640. Best iteration is:\n",
      "\t[508]\tvalid_set's l2: 2.93975e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.524, 'LightGBM': 0.476}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_53\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_54\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.80 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       260\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.67s of the 1.67s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 505. Best iteration is:\n",
      "\t[307]\tvalid_set's l2: 2.97768e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_54\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_55\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.75 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       261\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17299.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.63s of the 1.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.78353e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 484. Best iteration is:\n",
      "\t[426]\tvalid_set's l2: 2.95436e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_55\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_56\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.71 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       262\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.44s of the 1.44s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.82509e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 420. Best iteration is:\n",
      "\t[408]\tvalid_set's l2: 2.99004e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_56\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_57\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.67 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       263\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17299.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.77s of the 1.77s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 532. Best iteration is:\n",
      "\t[520]\tvalid_set's l2: 2.96184e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.562, 'LightGBM': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_57\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_58\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.88 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.62 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       264\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17301.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.77628e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.47s of the 0.47s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 85. Best iteration is:\n",
      "\t[85]\tvalid_set's l2: 3.24092e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_58\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_59\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.58 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       265\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.18s of the 2.18s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 681. Best iteration is:\n",
      "\t[674]\tvalid_set's l2: 3.0919e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_59\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_60\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.54 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       266\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.04s of the 2.04s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 624. Best iteration is:\n",
      "\t[611]\tvalid_set's l2: 3.0163e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.562, 'LightGBM': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_60\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_61\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.49 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       267\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.91s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 583. Best iteration is:\n",
      "\t[514]\tvalid_set's l2: 3.06962e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_61\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_62\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   868.45 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       268\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17294.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87934e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.07s of the 0.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's l2: 0.000819544\n",
      "\t-0.0008\t = Validation score   (-mean_squared_error)\n",
      "\t0.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_62\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_63\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.41 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       269\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.93158e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.38s of the 1.38s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 377. Best iteration is:\n",
      "\t[377]\tvalid_set's l2: 3.08467e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.562, 'LightGBM': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_63\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_64\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   868.36 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       270\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17291.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91213e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.73s of the 0.73s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 158. Best iteration is:\n",
      "\t[158]\tvalid_set's l2: 3.22649e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.611, 'LightGBM': 0.389}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_64\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_65\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   868.32 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       271\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17293.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9048e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1915. Best iteration is:\n",
      "\t[1747]\tvalid_set's l2: 2.88554e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_65\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_66\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.27 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       272\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9041e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.73s of the 0.73s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 156. Best iteration is:\n",
      "\t[150]\tvalid_set's l2: 3.23659e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_66\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_67\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.23 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       273\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17299.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.09s of the 2.08s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 634. Best iteration is:\n",
      "\t[633]\tvalid_set's l2: 3.0851e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_67\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_68\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.19 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       274\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.3s of the 2.3s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 726. Best iteration is:\n",
      "\t[722]\tvalid_set's l2: 3.11697e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_68\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_69\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.14 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       275\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.14s of the 2.14s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 649. Best iteration is:\n",
      "\t[582]\tvalid_set's l2: 3.08973e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_69\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_70\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.10 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       276\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.43 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85821e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1892. Best iteration is:\n",
      "\t[1594]\tvalid_set's l2: 2.84474e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_70\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_71\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.06 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       277\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.73s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.96s of the 3.96s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.89s of the 1.89s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 555. Best iteration is:\n",
      "\t[540]\tvalid_set's l2: 3.10898e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.27s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.588, 'LightGBM': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_71\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_72\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   868.01 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       278\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.7s of the 1.7s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 485. Best iteration is:\n",
      "\t[485]\tvalid_set's l2: 3.1244e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.611, 'LightGBM': 0.389}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_72\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_73\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.97 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       279\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89947e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.0s of the 1.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 246. Best iteration is:\n",
      "\t[246]\tvalid_set's l2: 3.15325e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_73\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_74\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.93 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       280\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.47s of the 1.47s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91366e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 404. Best iteration is:\n",
      "\t[403]\tvalid_set's l2: 3.14368e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_74\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_75\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.88 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       281\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.6s of the 1.6s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 455. Best iteration is:\n",
      "\t[448]\tvalid_set's l2: 3.15329e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.64, 'LightGBM': 0.36}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_75\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_76\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.84 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       282\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.24s of the 2.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 688. Best iteration is:\n",
      "\t[663]\tvalid_set's l2: 3.09253e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_76\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_77\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.79 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       283\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.68s of the 1.67s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 477. Best iteration is:\n",
      "\t[466]\tvalid_set's l2: 3.16607e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_77\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_78\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.75 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       284\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83216e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.91s of the 0.91s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 212. Best iteration is:\n",
      "\t[212]\tvalid_set's l2: 3.24359e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_78\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_79\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.71 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       285\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.86473e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.19s of the 1.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 307. Best iteration is:\n",
      "\t[305]\tvalid_set's l2: 3.21755e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_79\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_80\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.66 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       286\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.90501e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.22s of the 0.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 23. Best iteration is:\n",
      "\t[23]\tvalid_set's l2: 9.0428e-05\n",
      "\t-0.0001\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_80\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_81\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.62 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       287\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 586. Best iteration is:\n",
      "\t[584]\tvalid_set's l2: 3.09498e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.643, 'LightGBM': 0.357}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_81\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_82\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.58 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       288\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.11s of the 2.11s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 640. Best iteration is:\n",
      "\t[629]\tvalid_set's l2: 3.17983e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_82\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_83\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   867.53 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       289\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17294.29 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.6s of the 1.6s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89459e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 463. Best iteration is:\n",
      "\t[463]\tvalid_set's l2: 3.25409e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_83\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_84\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.49 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       290\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17295.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.19s of the 2.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 670. Best iteration is:\n",
      "\t[650]\tvalid_set's l2: 3.2252e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.643, 'LightGBM': 0.357}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_84\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_85\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.45 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       291\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 575. Best iteration is:\n",
      "\t[575]\tvalid_set's l2: 3.19665e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.636, 'LightGBM': 0.364}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_85\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_86\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.40 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       292\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17297.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.35s of the 2.35s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 725. Best iteration is:\n",
      "\t[704]\tvalid_set's l2: 3.21832e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_86\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_87\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.36 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       293\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.0s of the 2.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 603. Best iteration is:\n",
      "\t[583]\tvalid_set's l2: 3.22995e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_87\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_88\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.4%)\n",
      "Disk Space Avail:   867.32 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       294\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17292.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.46s of the 1.46s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98559e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 408. Best iteration is:\n",
      "\t[406]\tvalid_set's l2: 3.28071e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_88\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_89\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.27 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       295\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.98s of the 1.98s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 593. Best iteration is:\n",
      "\t[582]\tvalid_set's l2: 3.24081e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.619, 'LightGBM': 0.381}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_89\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_90\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.23 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       296\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.86s of the 1.86s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 556. Best iteration is:\n",
      "\t[556]\tvalid_set's l2: 3.30559e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_90\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_91\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.19 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       297\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89438e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1900. Best iteration is:\n",
      "\t[1496]\tvalid_set's l2: 2.88042e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_91\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_92\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.14 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       298\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17298.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.74s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.26s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.34s of the 2.34s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 703. Best iteration is:\n",
      "\t[702]\tvalid_set's l2: 3.26545e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.26s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_92\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_93\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.10 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       299\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17300.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.7s of the 1.7s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 473. Best iteration is:\n",
      "\t[471]\tvalid_set's l2: 3.29445e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.632, 'LightGBM': 0.368}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_93\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_94\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.87 GB / 21.50 GB (78.5%)\n",
      "Disk Space Avail:   867.06 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       300\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    17296.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.12676e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.22s of the 1.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 312. Best iteration is:\n",
      "\t[312]\tvalid_set's l2: 3.43413e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_94\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3h 50min 32s, sys: 22.2 s, total: 3h 50min 54s\n",
      "Wall time: 16min 45s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        #model_path = f'./autogluon_models/model_{i}'\n",
    "        model_path = f'../net95/autogluon_models_net95v1_20/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9685735c5423a2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T16:05:19.379751500Z",
     "start_time": "2024-02-23T16:05:19.343186800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_0/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_1/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_2/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_3/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_4/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_5/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_6/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_7/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_8/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_9/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_10/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_11/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_12/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_13/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_14/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_15/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_16/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_17/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_18/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_19/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_20/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_21/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_22/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_23/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_24/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_25/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_26/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_27/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_28/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_29/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_30/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_31/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_32/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_33/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_34/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_35/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_36/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_37/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_38/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_39/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_40/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_41/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_42/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_43/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_44/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_45/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_46/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_47/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_48/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_49/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_50/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_51/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_52/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_53/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_54/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_55/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_56/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_57/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_58/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_59/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_60/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_61/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_62/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_63/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_64/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_65/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_66/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_67/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_68/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_69/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_70/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_71/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_72/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_73/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_74/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_75/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_76/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_77/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_78/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n",
      "Model 18 loaded\n",
      "Model 19 loaded\n",
      "Model 20 loaded\n",
      "Model 21 loaded\n",
      "Model 22 loaded\n",
      "Model 23 loaded\n",
      "Model 24 loaded\n",
      "Model 25 loaded\n",
      "Model 26 loaded\n",
      "Model 27 loaded\n",
      "Model 28 loaded\n",
      "Model 29 loaded\n",
      "Model 30 loaded\n",
      "Model 31 loaded\n",
      "Model 32 loaded\n",
      "Model 33 loaded\n",
      "Model 34 loaded\n",
      "Model 35 loaded\n",
      "Model 36 loaded\n",
      "Model 37 loaded\n",
      "Model 38 loaded\n",
      "Model 39 loaded\n",
      "Model 40 loaded\n",
      "Model 41 loaded\n",
      "Model 42 loaded\n",
      "Model 43 loaded\n",
      "Model 44 loaded\n",
      "Model 45 loaded\n",
      "Model 46 loaded\n",
      "Model 47 loaded\n",
      "Model 48 loaded\n",
      "Model 49 loaded\n",
      "Model 50 loaded\n",
      "Model 51 loaded\n",
      "Model 52 loaded\n",
      "Model 53 loaded\n",
      "Model 54 loaded\n",
      "Model 55 loaded\n",
      "Model 56 loaded\n",
      "Model 57 loaded\n",
      "Model 58 loaded\n",
      "Model 59 loaded\n",
      "Model 60 loaded\n",
      "Model 61 loaded\n",
      "Model 62 loaded\n",
      "Model 63 loaded\n",
      "Model 64 loaded\n",
      "Model 65 loaded\n",
      "Model 66 loaded\n",
      "Model 67 loaded\n",
      "Model 68 loaded\n",
      "Model 69 loaded\n",
      "Model 70 loaded\n",
      "Model 71 loaded\n",
      "Model 72 loaded\n",
      "Model 73 loaded\n",
      "Model 74 loaded\n",
      "Model 75 loaded\n",
      "Model 76 loaded\n",
      "Model 77 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_79/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_80/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_81/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_82/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_83/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_84/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_85/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_86/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_87/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_88/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_89/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_90/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_91/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_92/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_93/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_94/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 78 loaded\n",
      "Model 79 loaded\n",
      "Model 80 loaded\n",
      "Model 81 loaded\n",
      "Model 82 loaded\n",
      "Model 83 loaded\n",
      "Model 84 loaded\n",
      "Model 85 loaded\n",
      "Model 86 loaded\n",
      "Model 87 loaded\n",
      "Model 88 loaded\n",
      "Model 89 loaded\n",
      "Model 90 loaded\n",
      "Model 91 loaded\n",
      "Model 92 loaded\n",
      "Model 93 loaded\n",
      "Model 94 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'../net95/autogluon_models_net95v1_20/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c91c44008cd32294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T16:09:14.938395Z",
     "start_time": "2024-02-23T16:08:39.688458300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "['std: 0.004040927562965566', 'std: 0.00745202236345188', 'std: 0.01135547367174476', 'std: 0.006869626179638906', 'std: 0.007565874562210814']\n"
     ]
    }
   ],
   "source": [
    "from net95.scenarios2 import get_data_by_scenario_and_case\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    print(f'SCENARIO {scenario}, CASE 1 VALIDATION')\n",
    "    s1_c1_data = get_data_by_scenario_and_case(scenario, 1, net_name='net95v1')\n",
    "    x = s1_c1_data[0]\n",
    "    x_hat = s1_c1_data[1]\n",
    "    y_all = s1_c1_data[2]\n",
    "    y_hat_all = s1_c1_data[3]\n",
    "    \n",
    "    estim = []\n",
    "    for i in range(num_output):\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        \n",
    "        predictor = models[i]\n",
    "        test_x = x_hat\n",
    "        test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "        test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "        \n",
    "        \n",
    "        preds = predictor.predict(test)\n",
    "        \n",
    "        estim.append(preds[0])\n",
    "        \n",
    "    pred = np.asarray(estim)\n",
    "    std_results.append(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e6dfce5-5eb4-425f-b2c9-909f535aa2f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net95/net_95_v2/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net95/net_95_v2/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29ea7dcc-8150-4218-a4e9-2f77e8ae897f",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 206\n",
    "num_output = 95\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b701530d-96e2-4d8c-8464-6fe567a0a29a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.56 GB / 21.50 GB (77.0%)\n",
      "Disk Space Avail:   867.01 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       206\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16953.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.14s of the 1.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.76859e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 275. Best iteration is:\n",
      "\t[263]\tvalid_set's l2: 3.09855e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.636, 'LightGBM': 0.364}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.97 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       207\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16926.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.92191e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.0s of the 1.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 218. Best iteration is:\n",
      "\t[218]\tvalid_set's l2: 3.10238e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.93 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       208\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16930.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.99s of the 1.99s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 521. Best iteration is:\n",
      "\t[287]\tvalid_set's l2: 3.0549e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.17s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.88 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       209\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16929.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.85s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.98s of the 3.98s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.83s of the 3.83s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.03s of the 1.02s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.8706e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 233. Best iteration is:\n",
      "\t[233]\tvalid_set's l2: 3.09111e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.1s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.15s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.84 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       210\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16931.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.0059e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1728. Best iteration is:\n",
      "\t[1456]\tvalid_set's l2: 2.98506e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.80 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       211\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16930.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.8s of the 1.8s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 501. Best iteration is:\n",
      "\t[443]\tvalid_set's l2: 3.00033e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.533, 'LightGBM': 0.467}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.75 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       212\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16928.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.79602e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1843. Best iteration is:\n",
      "\t[1613]\tvalid_set's l2: 2.78388e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.71 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       213\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16932.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89601e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.28s of the 1.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 313. Best iteration is:\n",
      "\t[149]\tvalid_set's l2: 2.95001e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.538, 'LightGBM': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.52 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.67 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       214\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16934.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.27s of the 1.27s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.82656e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 322. Best iteration is:\n",
      "\t[142]\tvalid_set's l2: 2.8047e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.52, 'LightGBMXT': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.52 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.62 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       215\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16932.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.85s of the 1.85s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.38s of the 0.38s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.66s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -41.57s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.538, 'LightGBMXT': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.62s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.51 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   866.27 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       216\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16928.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.1s of the 2.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.44s of the 0.44s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.79s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -40.63s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.545, 'LightGBMXT': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.69s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.50 GB / 21.50 GB (76.8%)\n",
      "Disk Space Avail:   865.92 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       217\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16920.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.01s of the 4.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.82914e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.75s of the 0.75s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 144. Best iteration is:\n",
      "\t[144]\tvalid_set's l2: 2.7513e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.542, 'LightGBMXT': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.48 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   865.88 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       218\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16892.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.46s of the 1.46s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 299. Best iteration is:\n",
      "\t[242]\tvalid_set's l2: 2.78288e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.591, 'LightGBMXT': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.84 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       219\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16871.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.01s of the 4.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.84s of the 3.84s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.2s of the 1.2s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 282. Best iteration is:\n",
      "\t[245]\tvalid_set's l2: 2.78606e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.17s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.615, 'LightGBMXT': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.79 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       220\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16875.70 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.44s of the 1.44s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 370. Best iteration is:\n",
      "\t[224]\tvalid_set's l2: 2.84315e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.522, 'LightGBMXT': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.75 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       221\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16871.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.8596e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.47s of the 0.47s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 72. Best iteration is:\n",
      "\t[72]\tvalid_set's l2: 3.57262e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.714, 'LightGBM': 0.286}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.71 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       222\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16870.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.96s of the 1.96s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.18s of the 0.18s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.46s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -40.57s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.545, 'LightGBMXT': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.62s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.49 GB / 21.50 GB (76.7%)\n",
      "Disk Space Avail:   865.36 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       223\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16907.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.85s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.99s of the 3.98s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.83s of the 3.83s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.92745e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.21s of the 0.21s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 11. Best iteration is:\n",
      "\t[11]\tvalid_set's l2: 0.000275286\n",
      "\t-0.0003\t = Validation score   (-mean_squared_error)\n",
      "\t0.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.15s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_17\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_18\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.32 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       224\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16875.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.98s of the 3.97s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.82s of the 3.81s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.6s of the 1.6s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.14s of the -0.02s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.07s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_18\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_19\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.27 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       225\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16874.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.14s of the 2.14s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 647. Best iteration is:\n",
      "\t[644]\tvalid_set's l2: 2.84086e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.52, 'LightGBMXT': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_19\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_20\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.23 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       226\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16872.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.73s of the 1.72s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.03s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.529, 'LightGBMXT': 0.471}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.07s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_20\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_21\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   865.19 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       227\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16870.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.81s of the 1.81s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.12s of the 0.12s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.3s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -39.47s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.52s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_21\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_22\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.49 GB / 21.50 GB (76.7%)\n",
      "Disk Space Avail:   864.84 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       228\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16909.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98554e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.18s of the 1.18s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 295. Best iteration is:\n",
      "\t[244]\tvalid_set's l2: 2.85596e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.56, 'LightGBMXT': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_22\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_23\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.79 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       229\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16875.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.92816e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the 0.03s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.01s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_23\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_24\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.75 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       230\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16872.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9745e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.19s of the 0.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 10. Best iteration is:\n",
      "\t[10]\tvalid_set's l2: 0.000312678\n",
      "\t-0.0003\t = Validation score   (-mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_24\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_25\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.71 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       231\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16873.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9884e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1771. Best iteration is:\n",
      "\t[1588]\tvalid_set's l2: 2.9793e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_25\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_26\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.66 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       232\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16873.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.86391e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.27s of the 1.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 317. Best iteration is:\n",
      "\t[315]\tvalid_set's l2: 3.00042e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.56, 'LightGBM': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_26\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_27\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.62 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       233\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16869.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9645e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.1s of the 1.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 257. Best iteration is:\n",
      "\t[249]\tvalid_set's l2: 3.10137e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_27\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_28\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   864.58 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       234\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16871.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.74s of the 1.74s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.25s of the 0.25s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -39.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.571, 'LightGBMXT': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_28\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_29\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.48 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   864.23 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       235\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16899.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.71s of the 1.71s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.15s of the 0.15s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.14s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -39.27s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.6, 'LightGBMXT': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.32s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_29\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_30\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.49 GB / 21.50 GB (76.7%)\n",
      "Disk Space Avail:   863.88 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       236\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16905.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.84925e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.5s of the 0.5s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 83. Best iteration is:\n",
      "\t[83]\tvalid_set's l2: 2.74919e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.545, 'LightGBMXT': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_30\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_31\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.83 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       237\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16872.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.84156e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.56s of the 0.56s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 107. Best iteration is:\n",
      "\t[106]\tvalid_set's l2: 2.62939e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.619, 'LightGBMXT': 0.381}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_31\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_32\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.79 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       238\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16868.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.86281e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.31s of the 1.31s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 361. Best iteration is:\n",
      "\t[124]\tvalid_set's l2: 2.55174e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.7, 'LightGBMXT': 0.3}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_32\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_33\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.75 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       239\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16869.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.84614e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.24s of the 1.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 352. Best iteration is:\n",
      "\t[170]\tvalid_set's l2: 2.65413e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.619, 'LightGBMXT': 0.381}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_33\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_34\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.70 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       240\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16865.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.55s of the 1.55s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.06s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.11s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_34\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_35\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.66 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       241\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16870.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.92555e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_35\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_36\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.62 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       242\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16869.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.82s of the 1.82s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.32s of the 0.32s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.86s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -39.83s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.5, 'LightGBMXT': 0.35, 'RandomForestMSE': 0.15}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.87s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_36\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_37\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.27 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       243\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16863.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.77467e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.71s of the 0.71s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 154. Best iteration is:\n",
      "\t[135]\tvalid_set's l2: 2.54607e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.647, 'LightGBMXT': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_37\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_38\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   863.22 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       244\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16873.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.94s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.59s of the 1.59s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.14s of the 0.14s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -39.91s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.542, 'LightGBMXT': 0.292, 'RandomForestMSE': 0.167}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_38\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_39\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.47 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   862.87 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       245\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16889.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.76006e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.37s of the 0.37s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 56. Best iteration is:\n",
      "\t[56]\tvalid_set's l2: 5.16886e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.85, 'LightGBM': 0.15}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_39\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_40\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   862.83 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       246\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16862.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.09s of the 2.09s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.62s of the 0.62s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.01s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -39.68s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.529, 'LightGBMXT': 0.412, 'RandomForestMSE': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.73s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_40\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_41\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   862.48 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       247\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16858.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.89s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.95s of the 3.95s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.79s of the 3.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.03s of the 2.03s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.27s of the 0.27s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.22s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.11s of the -41.23s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.55, 'LightGBMXT': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.28s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_41\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_42\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.49 GB / 21.50 GB (76.7%)\n",
      "Disk Space Avail:   862.13 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       248\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16880.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.97s of the 1.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.38s of the 0.38s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.67s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -39.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.55, 'LightGBMXT': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_42\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_43\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.48 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   861.78 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       249\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16885.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.43s of the 1.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 379. Best iteration is:\n",
      "\t[159]\tvalid_set's l2: 2.70144e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.579, 'LightGBMXT': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_43\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_44\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   861.74 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       250\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16878.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.58s of the 1.58s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.545, 'LightGBMXT': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_44\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_45\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   861.69 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       251\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16880.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.75s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.94s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83525e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.0s of the 1.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 246. Best iteration is:\n",
      "\t[222]\tvalid_set's l2: 2.70725e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.25s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.562, 'LightGBMXT': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_45\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_46\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.46 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   861.65 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16880.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.02s of the 2.02s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.48s of the 0.48s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.22s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -39.03s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.565, 'LightGBMXT': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_46\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_47\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.49 GB / 21.50 GB (76.7%)\n",
      "Disk Space Avail:   861.30 GB / 1006.85 GB (85.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       253\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16910.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.67s of the 1.67s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.07s of the 0.07s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -40.48s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.56, 'LightGBMXT': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.53s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_47\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_48\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   860.95 GB / 1006.85 GB (85.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       254\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16822.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.02s of the 2.02s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.11s of the 0.11s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.93s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -41.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.529, 'LightGBMXT': 0.471}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_48\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_49\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   860.60 GB / 1006.85 GB (85.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       255\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16827.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.01s of the 4.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83148e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.22s of the 0.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 15. Best iteration is:\n",
      "\t[15]\tvalid_set's l2: 0.000185583\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_49\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_50\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.41 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   860.56 GB / 1006.85 GB (85.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       256\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16819.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.8601e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.05s of the 1.05s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 242. Best iteration is:\n",
      "\t[173]\tvalid_set's l2: 2.7378e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.556, 'LightGBMXT': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_50\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_51\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   860.51 GB / 1006.85 GB (85.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       257\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16815.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.78s of the 1.78s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.31s of the 0.3s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.4s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -40.42s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.571, 'LightGBMXT': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_51\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_52\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   860.16 GB / 1006.85 GB (85.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       258\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16848.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.07s of the 2.07s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.5s of the 0.5s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t37.91s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.18s of the -37.72s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.542, 'LightGBMXT': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 42.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_52\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_53\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   859.82 GB / 1006.85 GB (85.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       259\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16846.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.49s of the 1.49s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83068e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.55, 'LightGBMXT': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_53\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_54\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   859.77 GB / 1006.85 GB (85.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       260\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16815.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.43s of the 0.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.97s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -39.86s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.571, 'LightGBMXT': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_54\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_55\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.42 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   859.42 GB / 1006.85 GB (85.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       261\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16817.79 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.17s of the 2.17s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 658. Best iteration is:\n",
      "\t[447]\tvalid_set's l2: 2.84011e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.533, 'LightGBMXT': 0.467}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_55\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_56\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   859.38 GB / 1006.85 GB (85.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       262\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.62s of the 1.62s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.07s of the 0.07s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.68s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -39.93s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.98s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_56\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_57\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   859.03 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       263\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16823.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.53s of the 1.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.02s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.533, 'LightGBMXT': 0.467}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.07s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_57\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_58\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.99 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       264\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16812.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.79836e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.11s of the 1.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 274. Best iteration is:\n",
      "\t[189]\tvalid_set's l2: 2.83421e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.522, 'LightGBM': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_58\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_59\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.95 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       265\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16815.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91913e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.29s of the 0.29s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 33. Best iteration is:\n",
      "\t[33]\tvalid_set's l2: 3.33395e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.96, 'LightGBM': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_59\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_60\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.90 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       266\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16813.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97191e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.98s of the 0.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 228. Best iteration is:\n",
      "\t[225]\tvalid_set's l2: 3.1293e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.56, 'LightGBM': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_60\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_61\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.86 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       267\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16812.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.99765e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the 0.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.03s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_61\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_62\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.81 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       268\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.99s of the 3.99s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.83s of the 3.83s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9624e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.64s of the 0.64s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 119. Best iteration is:\n",
      "\t[119]\tvalid_set's l2: 3.16615e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.14s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_62\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_63\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.77 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       269\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16812.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89711e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.26s of the 0.26s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 26. Best iteration is:\n",
      "\t[26]\tvalid_set's l2: 6.43249e-05\n",
      "\t-0.0001\t = Validation score   (-mean_squared_error)\n",
      "\t0.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_63\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_64\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.73 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       270\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.02024e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1868. Best iteration is:\n",
      "\t[1829]\tvalid_set's l2: 3.01117e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.01s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_64\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_65\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.68 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       271\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16814.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.86s of the 1.86s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 521. Best iteration is:\n",
      "\t[521]\tvalid_set's l2: 3.10802e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_65\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_66\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.64 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       272\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16807.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.19s of the 1.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.99646e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 284. Best iteration is:\n",
      "\t[281]\tvalid_set's l2: 3.14326e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.56, 'LightGBM': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_66\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_67\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.60 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       273\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16810.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.32s of the 2.32s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 684. Best iteration is:\n",
      "\t[669]\tvalid_set's l2: 3.11775e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.533, 'LightGBM': 0.467}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_67\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_68\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.55 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       274\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16814.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.63s of the 1.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 435. Best iteration is:\n",
      "\t[433]\tvalid_set's l2: 3.13198e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_68\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_69\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.51 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       275\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9983e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.97s of the 0.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 209. Best iteration is:\n",
      "\t[208]\tvalid_set's l2: 3.18446e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_69\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_70\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.37 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.47 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       276\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16788.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98252e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.33s of the 0.33s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 38. Best iteration is:\n",
      "\t[38]\tvalid_set's l2: 2.2178e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.96, 'LightGBM': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_70\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_71\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.42 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       277\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16810.31 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.53s of the 1.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 421. Best iteration is:\n",
      "\t[394]\tvalid_set's l2: 3.16837e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_71\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_72\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   858.38 GB / 1006.85 GB (85.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       278\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16843.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.39s of the 1.39s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97135e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 371. Best iteration is:\n",
      "\t[364]\tvalid_set's l2: 3.21637e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_72\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_73\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.34 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       279\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.86s of the 3.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.0263e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1851. Best iteration is:\n",
      "\t[1521]\tvalid_set's l2: 3.01422e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.17s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_73\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_74\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.29 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       280\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16808.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.88s of the 3.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.95076e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.71s of the 0.71s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 137. Best iteration is:\n",
      "\t[136]\tvalid_set's l2: 3.33852e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_74\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_75\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.25 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       281\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16811.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98681e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.27s of the 1.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 317. Best iteration is:\n",
      "\t[277]\tvalid_set's l2: 3.19022e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.35s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.588, 'LightGBM': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_75\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_76\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.40 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   858.21 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       282\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16810.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.58s of the 1.58s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 419. Best iteration is:\n",
      "\t[413]\tvalid_set's l2: 3.17408e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_76\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_77\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.16 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       283\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16807.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98323e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.23s of the 1.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 293. Best iteration is:\n",
      "\t[293]\tvalid_set's l2: 3.25456e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.611, 'LightGBM': 0.389}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_77\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_78\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.37 GB / 21.50 GB (76.1%)\n",
      "Disk Space Avail:   858.12 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       284\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16780.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98161e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.01s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_78\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_79\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.38 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.07 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       285\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16798.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.5s of the 1.5s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 394. Best iteration is:\n",
      "\t[389]\tvalid_set's l2: 3.23088e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_79\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_80\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   858.03 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       286\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.07279e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.92s of the 0.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 195. Best iteration is:\n",
      "\t[194]\tvalid_set's l2: 3.37318e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_80\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_81\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.99 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       287\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.87s of the 3.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.88314e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.65s of the 0.65s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[122]\tvalid_set's l2: 3.55645e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.19s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.708, 'LightGBM': 0.292}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_81\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_82\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.36 GB / 21.50 GB (76.1%)\n",
      "Disk Space Avail:   857.94 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       288\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16774.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.02112e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_82\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_83\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.38 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.90 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       289\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16798.24 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.89s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.94201e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.16s of the 1.16s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 282. Best iteration is:\n",
      "\t[280]\tvalid_set's l2: 3.32495e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.2s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.636, 'LightGBM': 0.364}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_83\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_84\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.86 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       290\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16800.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97782e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1757. Best iteration is:\n",
      "\t[1752]\tvalid_set's l2: 2.94532e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.99s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_84\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_85\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.38 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.81 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       291\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16798.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.88545e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.37s of the 0.37s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 53. Best iteration is:\n",
      "\t[53]\tvalid_set's l2: 8.58803e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.45s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.929, 'LightGBM': 0.071}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_85\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_86\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.77 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       292\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.08 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.93s of the 3.93s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91324e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.43s of the 0.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 64. Best iteration is:\n",
      "\t[64]\tvalid_set's l2: 5.55041e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.882, 'LightGBM': 0.118}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_86\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_87\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.72 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       293\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.90105e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.63s of the 0.62s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 120. Best iteration is:\n",
      "\t[120]\tvalid_set's l2: 3.74977e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.76, 'LightGBM': 0.24}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_87\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_88\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.68 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       294\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.12s of the 2.12s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 586. Best iteration is:\n",
      "\t[567]\tvalid_set's l2: 3.27978e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_88\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_89\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.38 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.64 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       295\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16798.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.76s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.51s of the 1.51s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 362. Best iteration is:\n",
      "\t[361]\tvalid_set's l2: 3.39119e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.24s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_89\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_90\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.39 GB / 21.50 GB (76.2%)\n",
      "Disk Space Avail:   857.60 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       296\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16801.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.18s of the 2.18s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 521. Best iteration is:\n",
      "\t[520]\tvalid_set's l2: 3.4082e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.26s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_90\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_91\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.22 GB / 21.50 GB (75.4%)\n",
      "Disk Space Avail:   857.55 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       297\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16628.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.9s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.93s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 3.92s of the 3.92s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.77s of the 3.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.88431e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.39s of the 0.39s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 52. Best iteration is:\n",
      "\t[52]\tvalid_set's l2: 1.00522e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.07s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.957, 'LightGBM': 0.043}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_91\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_92\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.24 GB / 21.50 GB (75.5%)\n",
      "Disk Space Avail:   857.51 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       298\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16654.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.91s of the 3.91s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9232e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.95s of the 0.95s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 200. Best iteration is:\n",
      "\t[199]\tvalid_set's l2: 3.68299e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.23s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.75, 'LightGBM': 0.25}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_92\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_93\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.21 GB / 21.50 GB (75.4%)\n",
      "Disk Space Avail:   857.47 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       299\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16622.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.89s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.00463e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.55s of the 0.55s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 92. Best iteration is:\n",
      "\t[92]\tvalid_set's l2: 4.60333e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.21s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.857, 'LightGBM': 0.143}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_93\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v2_20/model_94\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.21 GB / 21.50 GB (75.4%)\n",
      "Disk Space Avail:   857.42 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       300\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16621.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 3.9s of the 3.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.04s of the 2.04s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 485. Best iteration is:\n",
      "\t[485]\tvalid_set's l2: 3.41366e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.13s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.22s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v2_20/model_94\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4h 54min 22s, sys: 24.8 s, total: 4h 54min 47s\n",
      "Wall time: 20min 6s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        #model_path = f'./autogluon_models/model_{i}'\n",
    "        model_path = f'../net95/autogluon_models_net95v2_20/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "58ead61a-68b5-4c65-a05c-6a55ee12cec5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_0/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_1/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_2/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_3/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_4/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_5/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_6/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_7/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_8/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_9/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_10/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_11/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_12/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_13/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_14/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_15/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_16/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_17/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_18/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_19/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_20/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_21/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_22/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_23/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_24/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_25/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_26/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_27/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_28/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_29/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_30/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_31/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_32/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_33/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_34/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_35/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_36/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_37/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_38/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_39/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_40/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_41/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_42/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_43/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_44/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_45/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_46/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_47/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_48/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_49/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_50/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_51/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_52/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_53/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_54/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_55/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_56/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_57/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_58/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_59/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_60/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_61/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_62/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_63/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_64/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_65/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_66/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_67/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_68/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_69/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_70/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_71/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_72/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_73/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_74/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_75/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_76/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_77/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_78/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_79/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_80/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_81/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_82/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_83/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_84/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_85/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_86/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_87/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_88/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_89/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_90/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_91/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_92/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_93/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v2_20/model_94/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n",
      "Model 18 loaded\n",
      "Model 19 loaded\n",
      "Model 20 loaded\n",
      "Model 21 loaded\n",
      "Model 22 loaded\n",
      "Model 23 loaded\n",
      "Model 24 loaded\n",
      "Model 25 loaded\n",
      "Model 26 loaded\n",
      "Model 27 loaded\n",
      "Model 28 loaded\n",
      "Model 29 loaded\n",
      "Model 30 loaded\n",
      "Model 31 loaded\n",
      "Model 32 loaded\n",
      "Model 33 loaded\n",
      "Model 34 loaded\n",
      "Model 35 loaded\n",
      "Model 36 loaded\n",
      "Model 37 loaded\n",
      "Model 38 loaded\n",
      "Model 39 loaded\n",
      "Model 40 loaded\n",
      "Model 41 loaded\n",
      "Model 42 loaded\n",
      "Model 43 loaded\n",
      "Model 44 loaded\n",
      "Model 45 loaded\n",
      "Model 46 loaded\n",
      "Model 47 loaded\n",
      "Model 48 loaded\n",
      "Model 49 loaded\n",
      "Model 50 loaded\n",
      "Model 51 loaded\n",
      "Model 52 loaded\n",
      "Model 53 loaded\n",
      "Model 54 loaded\n",
      "Model 55 loaded\n",
      "Model 56 loaded\n",
      "Model 57 loaded\n",
      "Model 58 loaded\n",
      "Model 59 loaded\n",
      "Model 60 loaded\n",
      "Model 61 loaded\n",
      "Model 62 loaded\n",
      "Model 63 loaded\n",
      "Model 64 loaded\n",
      "Model 65 loaded\n",
      "Model 66 loaded\n",
      "Model 67 loaded\n",
      "Model 68 loaded\n",
      "Model 69 loaded\n",
      "Model 70 loaded\n",
      "Model 71 loaded\n",
      "Model 72 loaded\n",
      "Model 73 loaded\n",
      "Model 74 loaded\n",
      "Model 75 loaded\n",
      "Model 76 loaded\n",
      "Model 77 loaded\n",
      "Model 78 loaded\n",
      "Model 79 loaded\n",
      "Model 80 loaded\n",
      "Model 81 loaded\n",
      "Model 82 loaded\n",
      "Model 83 loaded\n",
      "Model 84 loaded\n",
      "Model 85 loaded\n",
      "Model 86 loaded\n",
      "Model 87 loaded\n",
      "Model 88 loaded\n",
      "Model 89 loaded\n",
      "Model 90 loaded\n",
      "Model 91 loaded\n",
      "Model 92 loaded\n",
      "Model 93 loaded\n",
      "Model 94 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'../net95/autogluon_models_net95v2_20/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "45c5e2c3-4efa-40d7-ae68-4096322c0767",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "['std: 0.005148013975434854', 'std: 0.0075832388957073876', 'std: 0.009188509684314476', 'std: 0.00391675084511186', 'std: 0.005424757231150596']\n"
     ]
    }
   ],
   "source": [
    "from net95.scenarios2 import get_data_by_scenario_and_case\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    print(f'SCENARIO {scenario}, CASE 1 VALIDATION')\n",
    "    s1_c1_data = get_data_by_scenario_and_case(scenario, 1, net_name='net95v2')\n",
    "    x = s1_c1_data[0]\n",
    "    x_hat = s1_c1_data[1]\n",
    "    y_all = s1_c1_data[2]\n",
    "    y_hat_all = s1_c1_data[3]\n",
    "    \n",
    "    estim = []\n",
    "    for i in range(num_output):\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        \n",
    "        predictor = models[i]\n",
    "        test_x = x_hat\n",
    "        test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "        test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "        \n",
    "        \n",
    "        preds = predictor.predict(test)\n",
    "        \n",
    "        estim.append(preds[0])\n",
    "        \n",
    "    pred = np.asarray(estim)\n",
    "    std_results.append(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3c460bba-3676-4ce9-947c-8d97c25f88d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net95/net_95_v3/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net95/net_95_v3/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1d1874df-0617-4a1e-b920-e6244a5dd9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 206\n",
    "num_output = 95\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e2bb79aa-40f3-4502-8dbd-6179d542c648",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.23 GB / 21.50 GB (75.5%)\n",
      "Disk Space Avail:   857.38 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       206\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16622.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.39s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.25907e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.16s of the 1.15s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 270. Best iteration is:\n",
      "\t[270]\tvalid_set's l2: 4.0496e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.739, 'LightGBM': 0.261}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.19 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.34 GB / 1006.85 GB (85.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       207\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16599.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.03s of the 2.02s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 549. Best iteration is:\n",
      "\t[533]\tvalid_set's l2: 3.60986e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.7, 'LightGBM': 0.3}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.18 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.29 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       208\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16593.75 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.1s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.10137e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1928. Best iteration is:\n",
      "\t[1922]\tvalid_set's l2: 3.07791e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.18 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.25 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       209\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16591.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.12297e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.69s of the 0.68s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 132. Best iteration is:\n",
      "\t[132]\tvalid_set's l2: 4.39565e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.8, 'LightGBM': 0.2}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.18 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.20 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       210\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16593.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.34s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.03886e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.45s of the 0.45s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 72. Best iteration is:\n",
      "\t[72]\tvalid_set's l2: 4.95693e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.85, 'LightGBM': 0.15}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.18 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.16 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       211\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16592.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.98993e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.95s of the 0.95s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 225. Best iteration is:\n",
      "\t[223]\tvalid_set's l2: 3.72519e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.739, 'LightGBM': 0.261}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.18 GB / 21.50 GB (75.3%)\n",
      "Disk Space Avail:   857.12 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       212\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16593.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.57s of the 1.57s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 413. Best iteration is:\n",
      "\t[412]\tvalid_set's l2: 3.33236e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   857.07 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       213\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16509.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.35s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.00047e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.3s of the 1.3s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 315. Best iteration is:\n",
      "\t[315]\tvalid_set's l2: 3.35973e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.15 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   857.03 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       214\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16563.23 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9619e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1883. Best iteration is:\n",
      "\t[1848]\tvalid_set's l2: 2.95566e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.12s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.15 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.98 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       215\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16563.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.07306e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2051. Best iteration is:\n",
      "\t[1638]\tvalid_set's l2: 3.06168e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.94 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       216\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.06501e-06\n",
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    16565.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.93s of the 1.93s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 556. Best iteration is:\n",
      "\t[550]\tvalid_set's l2: 3.16004e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.89 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       217\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16565.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.12641e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.31s of the 1.3s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 331. Best iteration is:\n",
      "\t[331]\tvalid_set's l2: 3.28634e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.85 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       218\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16564.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.11538e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.01s of the 1.01s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 236. Best iteration is:\n",
      "\t[236]\tvalid_set's l2: 3.32731e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.81 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       219\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16566.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.25s of the 4.24s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.06s of the 2.06s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 593. Best iteration is:\n",
      "\t[580]\tvalid_set's l2: 3.2487e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   856.76 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       220\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16568.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.19416e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.55s of the 0.55s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 93. Best iteration is:\n",
      "\t[93]\tvalid_set's l2: 4.10873e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.714, 'LightGBM': 0.286}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.72 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       221\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16547.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.11011e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1915. Best iteration is:\n",
      "\t[1768]\tvalid_set's l2: 3.09679e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.68 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       222\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16547.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.15723e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.43s of the 1.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 372. Best iteration is:\n",
      "\t[351]\tvalid_set's l2: 3.43869e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.63 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       223\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16548.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.2091e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1926. Best iteration is:\n",
      "\t[1758]\tvalid_set's l2: 3.18923e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_17\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_18\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.59 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       224\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16548.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.17834e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.38s of the 1.38s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 373. Best iteration is:\n",
      "\t[366]\tvalid_set's l2: 3.47359e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_18\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_19\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   856.54 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       225\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16545.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.39505e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2000. Best iteration is:\n",
      "\t[1933]\tvalid_set's l2: 3.37713e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_19\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_20\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.50 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       226\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16547.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.37779e-06\n",
      "Training for output #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.27739e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1869. Best iteration is:\n",
      "\t[1769]\tvalid_set's l2: 3.24203e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_20\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_21\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   856.45 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       227\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16545.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.09s of the 2.09s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 591. Best iteration is:\n",
      "\t[588]\tvalid_set's l2: 3.52914e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.55, 'LightGBM': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_21\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_22\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.41 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       228\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16547.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.33407e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.68s of the 0.68s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 137. Best iteration is:\n",
      "\t[137]\tvalid_set's l2: 4.07819e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_22\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_23\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   856.37 GB / 1006.85 GB (85.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       229\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16545.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.35798e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.99s of the 0.99s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 222. Best iteration is:\n",
      "\t[222]\tvalid_set's l2: 3.94984e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.07s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.64, 'LightGBM': 0.36}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_23\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_24\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.32 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       230\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16547.64 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.6812e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1988. Best iteration is:\n",
      "\t[1701]\tvalid_set's l2: 3.65362e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_24\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_25\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   856.28 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       231\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16545.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.39571e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2049. Best iteration is:\n",
      "\t[1803]\tvalid_set's l2: 3.37107e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_25\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_26\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.23 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       232\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.37947e-06\n",
      "Training for output #26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16548.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.74363e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.49s of the 1.49s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 407. Best iteration is:\n",
      "\t[397]\tvalid_set's l2: 4.18366e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_26\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_27\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   856.19 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       233\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16541.07 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.67884e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2046. Best iteration is:\n",
      "\t[1664]\tvalid_set's l2: 3.65722e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_27\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_28\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.14 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       234\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16548.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.6638e-06\n",
      "Training for output #28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97484e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.23s of the 1.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 343. Best iteration is:\n",
      "\t[320]\tvalid_set's l2: 2.99002e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_28\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_29\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.10 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       235\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16546.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.93959e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.5s of the 1.5s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 451. Best iteration is:\n",
      "\t[289]\tvalid_set's l2: 3.02059e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.538, 'LightGBM': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_29\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_30\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.14 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   856.06 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       236\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16549.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.49s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.43s of the 2.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.94s of the 0.93s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.9s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.49s of the -39.31s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.526, 'LightGBMXT': 0.474}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_30\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_31\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.15 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   855.71 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       237\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16560.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.05541e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.51s of the 1.51s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.02s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.56, 'LightGBMXT': 0.44}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_31\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_32\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   855.66 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       238\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16541.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.05217e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.61s of the 0.61s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 119. Best iteration is:\n",
      "\t[119]\tvalid_set's l2: 3.07856e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.18s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.52, 'LightGBM': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.23s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_32\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_33\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   855.62 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       239\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16539.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.9753e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.39s of the 1.38s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 390. Best iteration is:\n",
      "\t[365]\tvalid_set's l2: 3.08304e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.538, 'LightGBM': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_33\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_34\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   855.57 GB / 1006.85 GB (85.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       240\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16539.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.16s of the 2.16s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.53s of the 0.53s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.5s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -39.31s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.52, 'LightGBM': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.36s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_34\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_35\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.15 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   855.22 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       241\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16561.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.1s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.03s of the 2.03s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.43s of the 0.43s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.64s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -39.57s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.52, 'LightGBM': 0.44, 'RandomForestMSE': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_35\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_36\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.21 GB / 21.50 GB (75.4%)\n",
      "Disk Space Avail:   854.87 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       242\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16614.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.01s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.1s of the 2.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 669. Best iteration is:\n",
      "\t[662]\tvalid_set's l2: 3.1729e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_36\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_37\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.83 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       243\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16580.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.12164e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.74s of the 0.74s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 160. Best iteration is:\n",
      "\t[159]\tvalid_set's l2: 3.25538e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_37\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_38\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.79 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       244\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16570.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 576. Best iteration is:\n",
      "\t[575]\tvalid_set's l2: 3.29477e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_38\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_39\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.15 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   854.74 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       245\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16557.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.20144e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2107. Best iteration is:\n",
      "\t[1947]\tvalid_set's l2: 3.16999e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.17312e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_39\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_40\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.70 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       246\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16572.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.00245e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.67s of the 1.66s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 488. Best iteration is:\n",
      "\t[482]\tvalid_set's l2: 3.31628e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_40\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_41\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.65 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       247\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16576.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.2103e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.16s of the 0.16s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 3. Best iteration is:\n",
      "\t[3]\tvalid_set's l2: 0.000791971\n",
      "\t-0.0008\t = Validation score   (-mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_41\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_42\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.61 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       248\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16570.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.15863e-06\n",
      "[2000]\tvalid_set's l2: 3.14448e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2121. Best iteration is:\n",
      "\t[1618]\tvalid_set's l2: 3.13933e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_42\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_43\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.56 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       249\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16570.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.32881e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2072. Best iteration is:\n",
      "\t[1704]\tvalid_set's l2: 3.3118e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.31608e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_43\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_44\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   854.52 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       250\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16562.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.1s of the 2.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 643. Best iteration is:\n",
      "\t[640]\tvalid_set's l2: 3.49349e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_44\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_45\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.48 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       251\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16572.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.05s of the 2.05s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 633. Best iteration is:\n",
      "\t[600]\tvalid_set's l2: 3.4817e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_45\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_46\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.43 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16566.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.1s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.03s of the 2.03s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 617. Best iteration is:\n",
      "\t[613]\tvalid_set's l2: 3.47762e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_46\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_47\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.1%)\n",
      "Disk Space Avail:   854.39 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       253\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16567.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.7s of the 1.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.21294e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 494. Best iteration is:\n",
      "\t[490]\tvalid_set's l2: 3.52759e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_47\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_48\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.34 GB / 1006.85 GB (84.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       254\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16570.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.0s of the 2.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 592. Best iteration is:\n",
      "\t[592]\tvalid_set's l2: 3.46234e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.08s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_48\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_49\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.30 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       255\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16573.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.97s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.13s of the 2.12s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 658. Best iteration is:\n",
      "\t[651]\tvalid_set's l2: 3.50248e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_49\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_50\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.26 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       256\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16575.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.19555e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2107. Best iteration is:\n",
      "\t[2010]\tvalid_set's l2: 3.17215e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.17278e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_50\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_51\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.21 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       257\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16576.62 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.22034e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.43s of the 1.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 394. Best iteration is:\n",
      "\t[394]\tvalid_set's l2: 3.5726e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_51\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_52\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.17 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       258\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16572.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.73s of the 1.73s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.27268e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 495. Best iteration is:\n",
      "\t[489]\tvalid_set's l2: 3.54229e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_52\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_53\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.12 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       259\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16576.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.27823e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.54s of the 1.54s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 428. Best iteration is:\n",
      "\t[414]\tvalid_set's l2: 3.64584e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_53\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_54\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.08 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       260\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16574.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.07s of the 2.07s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 620. Best iteration is:\n",
      "\t[620]\tvalid_set's l2: 3.41779e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.55, 'LightGBM': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_54\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_55\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   854.04 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       261\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16575.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.7s of the 1.7s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.0744e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 501. Best iteration is:\n",
      "\t[491]\tvalid_set's l2: 3.5013e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.643, 'LightGBM': 0.357}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_55\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_56\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.16 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   853.99 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       262\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16570.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.09s of the 2.09s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 643. Best iteration is:\n",
      "\t[581]\tvalid_set's l2: 3.46378e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_56\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_57\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.17 GB / 21.50 GB (75.2%)\n",
      "Disk Space Avail:   853.95 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       263\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16576.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.27332e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.13s of the 1.13s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 186. Best iteration is:\n",
      "\t[186]\tvalid_set's l2: 3.5891e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.21s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.591, 'LightGBM': 0.409}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_57\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_58\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.91 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       264\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16527.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.39s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.08s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.29549e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.85s of the 0.85s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 192. Best iteration is:\n",
      "\t[191]\tvalid_set's l2: 3.59539e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_58\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_59\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.86 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       265\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16527.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.18237e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.23s of the 1.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 331. Best iteration is:\n",
      "\t[331]\tvalid_set's l2: 3.51716e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_59\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_60\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.82 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       266\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16520.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.22744e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.89s of the 0.89s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 209. Best iteration is:\n",
      "\t[208]\tvalid_set's l2: 3.83818e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_60\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_61\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.77 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       267\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16513.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.85s of the 1.85s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 544. Best iteration is:\n",
      "\t[532]\tvalid_set's l2: 3.70533e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.611, 'LightGBM': 0.389}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_61\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_62\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.73 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       268\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16513.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.21081e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.32s of the 1.31s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 347. Best iteration is:\n",
      "\t[344]\tvalid_set's l2: 3.7333e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.64, 'LightGBM': 0.36}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_62\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_63\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.69 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       269\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.99 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.36472e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.82s of the 0.82s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 178. Best iteration is:\n",
      "\t[178]\tvalid_set's l2: 3.96842e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_63\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_64\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.64 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       270\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16509.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.25697e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.35s of the 1.35s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 357. Best iteration is:\n",
      "\t[357]\tvalid_set's l2: 3.80844e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_64\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_65\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   853.60 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       271\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16506.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.38601e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2060. Best iteration is:\n",
      "\t[2057]\tvalid_set's l2: 3.36445e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.16s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.36718e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_65\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_66\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.55 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       272\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16533.11 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.35293e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2044. Best iteration is:\n",
      "\t[1953]\tvalid_set's l2: 3.33845e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_66\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_67\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.51 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       273\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.34086e-06\n",
      "Training for output #67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tAvailable Memory:                    16535.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.41826e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2070. Best iteration is:\n",
      "\t[2057]\tvalid_set's l2: 3.39314e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.39548e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_67\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_68\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.46 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       274\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16532.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.50797e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.1s of the 1.1s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 262. Best iteration is:\n",
      "\t[257]\tvalid_set's l2: 4.03721e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.632, 'LightGBM': 0.368}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_68\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_69\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.42 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       275\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16529.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.49106e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1978. Best iteration is:\n",
      "\t[1751]\tvalid_set's l2: 3.46247e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_69\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_70\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.37 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       276\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16533.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.61222e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2039. Best iteration is:\n",
      "\t[1875]\tvalid_set's l2: 3.59216e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_70\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_71\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.33 GB / 1006.85 GB (84.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       277\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.59791e-06\n",
      "Training for output #71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.55321e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2018. Best iteration is:\n",
      "\t[2018]\tvalid_set's l2: 3.5084e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_71\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_72\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.28 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       278\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16529.12 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.5096e-06\n",
      "Training for output #72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.5424e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2027. Best iteration is:\n",
      "\t[2018]\tvalid_set's l2: 3.51631e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_72\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_73\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.23 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       279\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16531.60 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.51682e-06\n",
      "Training for output #73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.63035e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.3s of the 1.3s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 338. Best iteration is:\n",
      "\t[335]\tvalid_set's l2: 4.16985e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_73\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_74\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.19 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       280\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16536.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.9s of the 1.9s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 552. Best iteration is:\n",
      "\t[552]\tvalid_set's l2: 4.11486e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.65, 'LightGBM': 0.35}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_74\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_75\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.15 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       281\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16533.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.61939e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.69s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.43s of the 0.43s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 67. Best iteration is:\n",
      "\t[67]\tvalid_set's l2: 6.92122e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.84, 'LightGBM': 0.16}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_75\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_76\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.10 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       282\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.18s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.24056e-06\n",
      "[2000]\tvalid_set's l2: 3.22178e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2133. Best iteration is:\n",
      "\t[1772]\tvalid_set's l2: 3.21643e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_76\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_77\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.06 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       283\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16536.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.99s of the 1.99s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 604. Best iteration is:\n",
      "\t[602]\tvalid_set's l2: 3.30804e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_77\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_78\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   853.01 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       284\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16533.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.25327e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.89s of the 0.89s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 203. Best iteration is:\n",
      "\t[202]\tvalid_set's l2: 3.40076e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_78\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_79\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   852.97 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       285\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16523.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.52s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.48s of the 4.48s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.38s of the 2.38s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 736. Best iteration is:\n",
      "\t[736]\tvalid_set's l2: 3.32411e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.48s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.538, 'LightGBM': 0.462}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_79\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_80\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   852.93 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       286\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16538.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.23s of the 2.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 680. Best iteration is:\n",
      "\t[598]\tvalid_set's l2: 3.3687e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_80\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_81\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   852.88 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       287\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16532.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.19963e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.65s of the 1.65s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 476. Best iteration is:\n",
      "\t[476]\tvalid_set's l2: 3.18111e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_81\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_82\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   852.84 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       288\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16536.39 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.15476e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.82s of the 0.82s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 182. Best iteration is:\n",
      "\t[181]\tvalid_set's l2: 3.27561e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_82\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_83\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   852.80 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       289\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16526.40 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.51s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.49s of the 4.49s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.10146e-06\n",
      "[2000]\tvalid_set's l2: 3.09624e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2115. Best iteration is:\n",
      "\t[2029]\tvalid_set's l2: 3.09514e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.49s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_83\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_84\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   852.75 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       290\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16521.67 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.42s of the 2.41s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.85s of the 0.85s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.15s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -38.57s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.6, 'LightGBMXT': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_84\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_85\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   852.40 GB / 1006.85 GB (84.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       291\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16521.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.76s of the 1.76s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.93873e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 0.1s of the 0.09s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t38.51s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -38.69s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.438, 'RandomForestMSE': 0.062}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.75s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_85\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_86\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   852.05 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       292\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16525.90 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.1013e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.82s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.29s of the 1.29s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 354. Best iteration is:\n",
      "\t[346]\tvalid_set's l2: 3.07733e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_86\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_87\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   852.01 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       293\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16505.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.32633e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.66s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.46s of the 1.45s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 391. Best iteration is:\n",
      "\t[391]\tvalid_set's l2: 3.28915e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.52, 'LightGBMXT': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_87\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_88\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.96 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       294\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.19089e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.58s of the 0.58s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 106. Best iteration is:\n",
      "\t[106]\tvalid_set's l2: 3.73842e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_88\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_89\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.92 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       295\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16497.26 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.2s of the 2.2s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 664. Best iteration is:\n",
      "\t[650]\tvalid_set's l2: 3.32714e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.562, 'LightGBM': 0.438}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_89\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_90\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.87 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       296\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.15s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.24636e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.4s of the 1.4s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 372. Best iteration is:\n",
      "\t[372]\tvalid_set's l2: 3.40974e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.545, 'LightGBM': 0.455}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_90\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_91\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.83 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       297\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16497.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.21983e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2070. Best iteration is:\n",
      "\t[1616]\tvalid_set's l2: 3.20337e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.14s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.21237e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_91\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_92\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.79 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       298\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.30208e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.34s of the 1.34s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 341. Best iteration is:\n",
      "\t[341]\tvalid_set's l2: 3.59565e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_92\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_93\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.74 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       299\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.36972e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.18s of the 0.17s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 7. Best iteration is:\n",
      "\t[7]\tvalid_set's l2: 0.000548151\n",
      "\t-0.0005\t = Validation score   (-mean_squared_error)\n",
      "\t0.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_93\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v3_20/model_94\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.70 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       300\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.53s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.47s of the 4.47s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.16s of the 4.15s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.50024e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.82s of the 0.82s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 178. Best iteration is:\n",
      "\t[176]\tvalid_set's l2: 4.11907e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.47s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.636, 'LightGBM': 0.364}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v3_20/model_94\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 2h 10min 10s, sys: 16.9 s, total: 2h 10min 27s\n",
      "Wall time: 11min 26s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        #model_path = f'./autogluon_models/model_{i}'\n",
    "        model_path = f'../net95/autogluon_models_net95v3_20/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "07f64fb8-ce28-4f68-860d-bb200d069669",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_0/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_1/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_2/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_3/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_4/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_5/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_6/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_7/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_8/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_9/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_10/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_11/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_12/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_13/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_14/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_15/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_16/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_17/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_18/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_19/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_20/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_21/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_22/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_23/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_24/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_25/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_26/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_27/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_28/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_29/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_30/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_31/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_32/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_33/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_34/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_35/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_36/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_37/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_38/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_39/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_40/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_41/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_42/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_43/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_44/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_45/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_46/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_47/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_48/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_49/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_50/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_51/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_52/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_53/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_54/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_55/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_56/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_57/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_58/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_59/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_60/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_61/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_62/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_63/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_64/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_65/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_66/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_67/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_68/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_69/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_70/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_71/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_72/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_73/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_74/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_75/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_76/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_77/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_78/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_79/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_80/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_81/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_82/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_83/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_84/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_85/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_86/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_87/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_88/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_89/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_90/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_91/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_92/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_93/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v3_20/model_94/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n",
      "Model 18 loaded\n",
      "Model 19 loaded\n",
      "Model 20 loaded\n",
      "Model 21 loaded\n",
      "Model 22 loaded\n",
      "Model 23 loaded\n",
      "Model 24 loaded\n",
      "Model 25 loaded\n",
      "Model 26 loaded\n",
      "Model 27 loaded\n",
      "Model 28 loaded\n",
      "Model 29 loaded\n",
      "Model 30 loaded\n",
      "Model 31 loaded\n",
      "Model 32 loaded\n",
      "Model 33 loaded\n",
      "Model 34 loaded\n",
      "Model 35 loaded\n",
      "Model 36 loaded\n",
      "Model 37 loaded\n",
      "Model 38 loaded\n",
      "Model 39 loaded\n",
      "Model 40 loaded\n",
      "Model 41 loaded\n",
      "Model 42 loaded\n",
      "Model 43 loaded\n",
      "Model 44 loaded\n",
      "Model 45 loaded\n",
      "Model 46 loaded\n",
      "Model 47 loaded\n",
      "Model 48 loaded\n",
      "Model 49 loaded\n",
      "Model 50 loaded\n",
      "Model 51 loaded\n",
      "Model 52 loaded\n",
      "Model 53 loaded\n",
      "Model 54 loaded\n",
      "Model 55 loaded\n",
      "Model 56 loaded\n",
      "Model 57 loaded\n",
      "Model 58 loaded\n",
      "Model 59 loaded\n",
      "Model 60 loaded\n",
      "Model 61 loaded\n",
      "Model 62 loaded\n",
      "Model 63 loaded\n",
      "Model 64 loaded\n",
      "Model 65 loaded\n",
      "Model 66 loaded\n",
      "Model 67 loaded\n",
      "Model 68 loaded\n",
      "Model 69 loaded\n",
      "Model 70 loaded\n",
      "Model 71 loaded\n",
      "Model 72 loaded\n",
      "Model 73 loaded\n",
      "Model 74 loaded\n",
      "Model 75 loaded\n",
      "Model 76 loaded\n",
      "Model 77 loaded\n",
      "Model 78 loaded\n",
      "Model 79 loaded\n",
      "Model 80 loaded\n",
      "Model 81 loaded\n",
      "Model 82 loaded\n",
      "Model 83 loaded\n",
      "Model 84 loaded\n",
      "Model 85 loaded\n",
      "Model 86 loaded\n",
      "Model 87 loaded\n",
      "Model 88 loaded\n",
      "Model 89 loaded\n",
      "Model 90 loaded\n",
      "Model 91 loaded\n",
      "Model 92 loaded\n",
      "Model 93 loaded\n",
      "Model 94 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'../net95/autogluon_models_net95v3_20/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "da450131-825d-4d1d-9d94-1ae72b05f764",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "['std: 0.0016418230559159383', 'std: 0.001557316467748427', 'std: 0.0023671720969766848', 'std: 0.0008029049706715936', 'std: 0.0011580966936310816']\n"
     ]
    }
   ],
   "source": [
    "from net95.scenarios2 import get_data_by_scenario_and_case\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    print(f'SCENARIO {scenario}, CASE 1 VALIDATION')\n",
    "    s1_c1_data = get_data_by_scenario_and_case(scenario, 1, net_name='net95v3')\n",
    "    x = s1_c1_data[0]\n",
    "    x_hat = s1_c1_data[1]\n",
    "    y_all = s1_c1_data[2]\n",
    "    y_hat_all = s1_c1_data[3]\n",
    "    \n",
    "    estim = []\n",
    "    for i in range(num_output):\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        \n",
    "        predictor = models[i]\n",
    "        test_x = x_hat\n",
    "        test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "        test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "        \n",
    "        \n",
    "        preds = predictor.predict(test)\n",
    "        \n",
    "        estim.append(preds[0])\n",
    "        \n",
    "    pred = np.asarray(estim)\n",
    "    std_results.append(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "03b9c7ef-8075-4d37-b671-a4c3e2ae32d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net95/net_95_v4/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net95/net_95_v4/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3b5b81ae-e86e-4ff5-af7b-89ebf9430fb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_input = 206\n",
    "num_output = 95\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "da51387b-3296-4d34-b0c6-66353f37b077",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.65 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       206\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16519.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.11259e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1868. Best iteration is:\n",
      "\t[1868]\tvalid_set's l2: 4.06882e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.61 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       207\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.1898e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1887. Best iteration is:\n",
      "\t[1847]\tvalid_set's l2: 4.101e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.56 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       208\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16502.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.30549e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1933. Best iteration is:\n",
      "\t[1903]\tvalid_set's l2: 4.24445e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.52 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       209\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.14224e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1916. Best iteration is:\n",
      "\t[1914]\tvalid_set's l2: 4.07799e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.47 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       210\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.05663e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1938. Best iteration is:\n",
      "\t[1938]\tvalid_set's l2: 3.98638e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.43 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       211\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16499.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.32514e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1981. Best iteration is:\n",
      "\t[1981]\tvalid_set's l2: 4.26615e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.38 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       212\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16498.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.94396e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1958. Best iteration is:\n",
      "\t[1920]\tvalid_set's l2: 3.90427e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.33 GB / 1006.85 GB (84.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       213\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.56 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.78134e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.9s of the 0.9s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 212. Best iteration is:\n",
      "\t[212]\tvalid_set's l2: 4.25215e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.706, 'LightGBM': 0.294}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.29 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       214\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16499.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 564. Best iteration is:\n",
      "\t[552]\tvalid_set's l2: 4.05513e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.25 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       215\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16502.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.39s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.64227e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1970. Best iteration is:\n",
      "\t[1964]\tvalid_set's l2: 3.5815e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.20 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       216\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16502.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.69787e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2012. Best iteration is:\n",
      "\t[1923]\tvalid_set's l2: 3.66032e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.16 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       217\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16504.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.66267e-06\n",
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.65765e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.22s of the 1.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 310. Best iteration is:\n",
      "\t[309]\tvalid_set's l2: 3.95776e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.11 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       218\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.60898e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.76s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.24s of the 1.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 310. Best iteration is:\n",
      "\t[305]\tvalid_set's l2: 3.89074e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   851.07 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       219\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16503.94 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.50826e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1984. Best iteration is:\n",
      "\t[1963]\tvalid_set's l2: 3.47344e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   851.02 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       220\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.73 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.69s of the 1.69s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 471. Best iteration is:\n",
      "\t[468]\tvalid_set's l2: 3.88694e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.55, 'LightGBM': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   850.98 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       221\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16496.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.54699e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1645. Best iteration is:\n",
      "\t[1637]\tvalid_set's l2: 3.51621e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.96 GB / 21.50 GB (74.2%)\n",
      "Disk Space Avail:   850.93 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       222\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16366.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.62804e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2037. Best iteration is:\n",
      "\t[1967]\tvalid_set's l2: 3.5934e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.00 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.89 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       223\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16405.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.59532e-06\n",
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.71012e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.35s of the 0.34s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 49. Best iteration is:\n",
      "\t[49]\tvalid_set's l2: 1.505e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.933, 'LightGBM': 0.067}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_17\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_18\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.99 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.84 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       224\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16398.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #18\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.73018e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.79s of the 0.78s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 160. Best iteration is:\n",
      "\t[160]\tvalid_set's l2: 4.36401e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_18\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_19\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.00 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.80 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       225\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16403.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.75337e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1980. Best iteration is:\n",
      "\t[1514]\tvalid_set's l2: 3.74505e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_19\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_20\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.99 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.76 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       226\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16397.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.39s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.81464e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2047. Best iteration is:\n",
      "\t[2008]\tvalid_set's l2: 3.78863e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.18s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_20\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_21\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.00 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.71 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       227\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.78971e-06\n",
      "Training for output #21\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16407.38 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.85s of the 1.85s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 526. Best iteration is:\n",
      "\t[515]\tvalid_set's l2: 4.02068e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_21\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_22\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.99 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.67 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       228\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16398.15 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #22\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.42s of the 1.42s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.77265e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 341. Best iteration is:\n",
      "\t[338]\tvalid_set's l2: 4.24924e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_22\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_23\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.00 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.62 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       229\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16405.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #23\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.01s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.41821e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1888. Best iteration is:\n",
      "\t[1818]\tvalid_set's l2: 3.40805e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.33s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_23\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_24\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.00 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.58 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       230\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16401.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #24\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.63s of the 1.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.70199e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 418. Best iteration is:\n",
      "\t[418]\tvalid_set's l2: 4.49954e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_24\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_25\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       15.99 GB / 21.50 GB (74.4%)\n",
      "Disk Space Avail:   850.53 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       231\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16395.48 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.64413e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.25s of the 1.25s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 284. Best iteration is:\n",
      "\t[284]\tvalid_set's l2: 4.69116e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.68, 'LightGBM': 0.32}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_25\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_26\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.49 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       232\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16530.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #26\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.83s of the 1.83s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 516. Best iteration is:\n",
      "\t[514]\tvalid_set's l2: 4.76323e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.12s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_26\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_27\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   850.45 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       233\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16520.55 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #27\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.35s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.84221e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.65s of the 0.65s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 126. Best iteration is:\n",
      "\t[126]\tvalid_set's l2: 6.01299e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.72s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.75, 'LightGBM': 0.25}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_27\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_28\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   850.40 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       234\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16516.00 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.49689e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.62s of the 0.61s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 107. Best iteration is:\n",
      "\t[107]\tvalid_set's l2: 4.34264e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.69s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.714, 'LightGBM': 0.286}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_28\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_29\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   850.36 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       235\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16517.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #29\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.54252e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.19s of the 0.19s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 9. Best iteration is:\n",
      "\t[9]\tvalid_set's l2: 0.00046106\n",
      "\t-0.0005\t = Validation score   (-mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_29\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_30\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   850.32 GB / 1006.85 GB (84.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       236\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16516.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #30\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.18s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.57s of the 1.57s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.59311e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 451. Best iteration is:\n",
      "\t[451]\tvalid_set's l2: 3.79199e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.65s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.556, 'LightGBM': 0.444}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_30\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_31\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   850.27 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       237\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16511.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #31\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.71197e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1917. Best iteration is:\n",
      "\t[1897]\tvalid_set's l2: 3.67926e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_31\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_32\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.23 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       238\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16531.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.66s of the 1.66s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 498. Best iteration is:\n",
      "\t[477]\tvalid_set's l2: 3.79096e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.542, 'LightGBM': 0.458}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_32\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_33\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.18 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       239\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.61s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.39s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.73443e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.07s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.39s of the -0.05s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_33\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_34\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.14 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       240\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16536.30 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #34\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.69066e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.11s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.08s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_34\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_35\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.10 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       241\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16536.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.58561e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.49s of the 0.49s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 82. Best iteration is:\n",
      "\t[82]\tvalid_set's l2: 4.65754e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.08s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_35\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_36\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.05 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       242\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #36\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.68s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.32s of the 4.32s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.01s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.70805e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.97s of the 0.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 230. Best iteration is:\n",
      "\t[230]\tvalid_set's l2: 4.16516e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.32s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.588, 'LightGBM': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_36\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_37\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   850.01 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       243\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16531.77 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.35s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.04s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.73741e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1979. Best iteration is:\n",
      "\t[1969]\tvalid_set's l2: 3.69951e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.14s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_37\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_38\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.96 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       244\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16531.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.62935e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.35s of the 0.35s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 48. Best iteration is:\n",
      "\t[48]\tvalid_set's l2: 1.75324e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.96, 'LightGBM': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_38\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_39\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.92 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       245\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.73458e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.12s of the 1.11s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 273. Best iteration is:\n",
      "\t[273]\tvalid_set's l2: 4.15069e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.583, 'LightGBM': 0.417}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_39\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_40\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.87 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       246\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.04s of the 4.04s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.76977e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.4s of the 0.4s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 61. Best iteration is:\n",
      "\t[61]\tvalid_set's l2: 8.65754e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.842, 'LightGBM': 0.158}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_40\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_41\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.83 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       247\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16534.33 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #41\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.87597e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.69s of the 0.69s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 140. Best iteration is:\n",
      "\t[140]\tvalid_set's l2: 4.52075e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.76s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.619, 'LightGBM': 0.381}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_41\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_42\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.79 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       248\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16535.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #42\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.59177e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.06s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.99s of the 0.99s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 237. Best iteration is:\n",
      "\t[237]\tvalid_set's l2: 4.43524e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.06s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_42\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_43\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.74 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       249\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16532.34 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #43\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.8796e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1973. Best iteration is:\n",
      "\t[1524]\tvalid_set's l2: 3.85282e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_43\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_44\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.13 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.70 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       250\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16533.49 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.86441e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1869. Best iteration is:\n",
      "\t[1492]\tvalid_set's l2: 3.84397e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_44\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_45\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.12 GB / 21.50 GB (75.0%)\n",
      "Disk Space Avail:   849.65 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       251\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16530.72 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #45\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.4s of the 4.4s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.96s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.09s of the 1.09s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.8386e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 251. Best iteration is:\n",
      "\t[251]\tvalid_set's l2: 4.63351e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.4s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_45\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_46\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   849.61 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       252\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16496.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.01s of the 4.01s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.86188e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.51s of the 0.51s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 92. Best iteration is:\n",
      "\t[92]\tvalid_set's l2: 5.64484e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.739, 'LightGBM': 0.261}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_46\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_47\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.57 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       253\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.72567e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.63s of the 0.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 119. Best iteration is:\n",
      "\t[119]\tvalid_set's l2: 5.33291e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.708, 'LightGBM': 0.292}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_47\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_48\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.52 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       254\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16508.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #48\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.70196e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.49s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.53s of the 0.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 95. Best iteration is:\n",
      "\t[95]\tvalid_set's l2: 5.60647e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.737, 'LightGBM': 0.263}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_48\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_49\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.48 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       255\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16508.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #49\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.81488e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2002. Best iteration is:\n",
      "\t[1802]\tvalid_set's l2: 3.79898e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_49\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_50\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.43 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       256\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16508.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.80476e-06\n",
      "Training for output #50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.84792e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.74s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.35s of the 1.35s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 353. Best iteration is:\n",
      "\t[353]\tvalid_set's l2: 4.5636e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_50\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_51\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.39 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       257\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.36s of the 4.36s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.90255e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.83s of the 0.83s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 183. Best iteration is:\n",
      "\t[181]\tvalid_set's l2: 5.03633e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.36s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.667, 'LightGBM': 0.333}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_51\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_52\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.35 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       258\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16511.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #52\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.66s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.34s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.18s of the 4.17s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.08s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.92s of the 1.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 542. Best iteration is:\n",
      "\t[542]\tvalid_set's l2: 4.39594e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.34s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.609, 'LightGBM': 0.391}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_52\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_53\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.30 GB / 1006.85 GB (84.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       259\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16504.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #53\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.17s of the 4.17s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.01s of the 4.0s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.89334e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.23s of the 1.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 314. Best iteration is:\n",
      "\t[312]\tvalid_set's l2: 4.6228e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.33s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.625, 'LightGBM': 0.375}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_53\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_54\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.26 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       260\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.71 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.7s of the 1.69s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.93222e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 470. Best iteration is:\n",
      "\t[466]\tvalid_set's l2: 4.2735e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.571, 'LightGBM': 0.429}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_54\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_55\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.22 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       261\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.69s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.31s of the 4.31s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.16s of the 4.16s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.0s of the 4.0s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.8129e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.24s of the 0.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 24. Best iteration is:\n",
      "\t[24]\tvalid_set's l2: 0.000159461\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.31s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_55\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_56\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.17 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       262\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16509.52 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #56\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.86849e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.58s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.45s of the 0.45s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 74. Best iteration is:\n",
      "\t[74]\tvalid_set's l2: 5.7132e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.727, 'LightGBM': 0.273}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_56\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_57\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   849.13 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       263\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16505.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #57\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.04s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.83097e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.42s of the 0.41s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 69. Best iteration is:\n",
      "\t[69]\tvalid_set's l2: 6.43991e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.765, 'LightGBM': 0.235}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_57\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_58\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   849.08 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       264\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16497.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #58\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.2s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 2.33s of the 2.32s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 727. Best iteration is:\n",
      "\t[727]\tvalid_set's l2: 4.13602e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.16s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_58\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_59\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   849.04 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       265\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16494.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #59\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.35s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.2s of the 4.2s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.10156e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2021. Best iteration is:\n",
      "\t[1639]\tvalid_set's l2: 4.07003e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_59\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_60\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   849.00 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       266\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16493.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.07736e-06\n",
      "Training for output #60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.08359e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2064. Best iteration is:\n",
      "\t[1806]\tvalid_set's l2: 4.05518e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.15s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.06028e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_60\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_61\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.95 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       267\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16493.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #61\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.04503e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2086. Best iteration is:\n",
      "\t[1839]\tvalid_set's l2: 3.98683e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 3.9966e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_61\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_62\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.08 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.90 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       268\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16491.81 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #62\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.15869e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2026. Best iteration is:\n",
      "\t[2009]\tvalid_set's l2: 4.12166e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_62\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_63\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.08 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.86 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       269\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16483.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.12282e-06\n",
      "Training for output #63\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.6s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.4s of the 4.4s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.11782e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.13s of the 0.13s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\tvalid_set's l2: 0.00180571\n",
      "\t-0.0018\t = Validation score   (-mean_squared_error)\n",
      "\t0.22s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.4s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_63\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_64\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.81 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       270\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16510.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.35s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.04s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.28028e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.05s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_64\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_65\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.77 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       271\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16502.45 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #65\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.34734e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2090. Best iteration is:\n",
      "\t[2087]\tvalid_set's l2: 4.31789e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.32174e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.17s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_65\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_66\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.72 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       272\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #66\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.1671e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2054. Best iteration is:\n",
      "\t[2054]\tvalid_set's l2: 4.14517e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.15005e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_66\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_67\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.68 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       273\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16503.85 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #67\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.72s of the 1.72s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.1723e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 519. Best iteration is:\n",
      "\t[515]\tvalid_set's l2: 4.73861e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_67\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_68\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.63 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       274\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16513.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #68\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.27891e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.46s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.57s of the 0.57s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 108. Best iteration is:\n",
      "\t[108]\tvalid_set's l2: 5.18056e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.632, 'LightGBM': 0.368}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_68\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_69\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.59 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       275\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16505.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #69\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.36s of the 4.36s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.49632e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1975. Best iteration is:\n",
      "\t[1718]\tvalid_set's l2: 4.44035e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.13s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.36s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_69\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_70\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.08 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.55 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       276\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16491.03 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.25s of the 4.25s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.54126e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.48s of the 1.48s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 415. Best iteration is:\n",
      "\t[376]\tvalid_set's l2: 4.98493e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_70\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_71\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.50 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       277\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16494.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.65s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.35s of the 4.34s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.19s of the 4.19s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.02s of the 4.02s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.92514e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.33s of the 0.33s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 44. Best iteration is:\n",
      "\t[44]\tvalid_set's l2: 3.34238e-05\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.35s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.941, 'LightGBM': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_71\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_72\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.08 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.46 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       278\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16491.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #72\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.52959e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2026. Best iteration is:\n",
      "\t[1782]\tvalid_set's l2: 4.50848e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_72\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_73\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.41 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       279\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16496.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.51522e-06\n",
      "Training for output #73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.59s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.41s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.24s of the 4.24s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.08s of the 4.08s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.87863e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 1.25s of the 1.25s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 334. Best iteration is:\n",
      "\t[321]\tvalid_set's l2: 4.99115e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.41s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.52, 'LightGBM': 0.48}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_73\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_74\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.37 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       280\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16492.80 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.23s of the 4.23s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.07s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.62084e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2029. Best iteration is:\n",
      "\t[1817]\tvalid_set's l2: 4.60736e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_74\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_75\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.08 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.32 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       281\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16488.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.61011e-06\n",
      "Training for output #75\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.56s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.44s of the 4.44s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.70491e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1989. Best iteration is:\n",
      "\t[1853]\tvalid_set's l2: 4.6772e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.44s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_75\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_76\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.28 GB / 1006.85 GB (84.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       282\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16517.51 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #76\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.973e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.72s of the 0.72s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 152. Best iteration is:\n",
      "\t[135]\tvalid_set's l2: 4.64504e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.13s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_76\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_77\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.23 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       283\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.36 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #77\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.63s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.37s of the 4.37s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.21622e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2030. Best iteration is:\n",
      "\t[1901]\tvalid_set's l2: 4.1909e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.16s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.37s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_77\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_78\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   848.19 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       284\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16500.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.1915e-06\n",
      "Training for output #78\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.43s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.12s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.16477e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2022. Best iteration is:\n",
      "\t[1572]\tvalid_set's l2: 4.14564e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_78\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_79\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.14 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       285\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16508.28 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.15825e-06\n",
      "Training for output #79\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.28963e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2014. Best iteration is:\n",
      "\t[1937]\tvalid_set's l2: 4.2679e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.25s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.16s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_79\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_80\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.10 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       286\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16515.02 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.26922e-06\n",
      "Training for output #80\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.57s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.43s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.11s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.14889e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2015. Best iteration is:\n",
      "\t[1967]\tvalid_set's l2: 4.09789e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.43s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_80\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_81\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.05 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       287\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16507.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.10077e-06\n",
      "Training for output #81\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.28s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.25807e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2056. Best iteration is:\n",
      "\t[1677]\tvalid_set's l2: 4.23666e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_81\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_82\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   848.01 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       288\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.24574e-06\n",
      "Training for output #82\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16514.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.21086e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2036. Best iteration is:\n",
      "\t[1596]\tvalid_set's l2: 4.18345e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_82\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_83\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   847.96 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       289\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16506.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.18711e-06\n",
      "Training for output #83\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.28s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.38811e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2119. Best iteration is:\n",
      "\t[1632]\tvalid_set's l2: 4.34272e-06\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.34953e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.22s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.13s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_83\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_84\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   847.92 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       290\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16515.16 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.07s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.53775e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.59s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.44s of the 0.44s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 79. Best iteration is:\n",
      "\t[79]\tvalid_set's l2: 6.99969e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.706, 'LightGBM': 0.294}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_84\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_85\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   847.87 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       291\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16507.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #85\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.21s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.11s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.03s of the 4.03s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.25652e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the 0.0s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.04s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_85\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_86\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.83 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       292\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16501.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.71849e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2000. Best iteration is:\n",
      "\t[1999]\tvalid_set's l2: 4.65813e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_86\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_87\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.78 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       293\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16497.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 4.65828e-06\n",
      "Training for output #87\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.45s of the 4.45s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.29s of the 4.29s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.13s of the 4.13s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.58852e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1997. Best iteration is:\n",
      "\t[1980]\tvalid_set's l2: 4.52175e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.45s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_87\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_88\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.11 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   847.74 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       294\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16520.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #88\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.41s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.09s of the 4.09s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.68896e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.27s of the 0.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 28. Best iteration is:\n",
      "\t[28]\tvalid_set's l2: 0.000137097\n",
      "\t-0.0001\t = Validation score   (-mean_squared_error)\n",
      "\t0.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_88\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_89\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.10 GB / 21.50 GB (74.9%)\n",
      "Disk Space Avail:   847.69 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       295\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16506.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.64s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.36s of the 4.36s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.21s of the 4.2s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.05s of the 4.05s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.58038e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1993. Best iteration is:\n",
      "\t[1987]\tvalid_set's l2: 4.55027e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.15s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.36s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_89\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_90\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.65 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       296\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16494.74 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #90\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.54s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.46s of the 4.46s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.3s of the 4.3s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.14s of the 4.14s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.48621e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1993. Best iteration is:\n",
      "\t[1722]\tvalid_set's l2: 4.45891e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.46s of the -0.14s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_90\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_91\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.60 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       297\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16496.83 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.26s of the 4.26s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.1s of the 4.1s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.40364e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.87s of the 0.87s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 194. Best iteration is:\n",
      "\t[194]\tvalid_set's l2: 6.28479e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.1s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.708, 'LightGBM': 0.292}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_91\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_92\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.56 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       298\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16498.37 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #92\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.67s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.33s of the 4.33s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.17s of the 4.16s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.01s of the 4.01s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.59917e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.35s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.62s of the 0.62s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 122. Best iteration is:\n",
      "\t[122]\tvalid_set's l2: 6.55093e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.33s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.692, 'LightGBM': 0.308}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_92\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_93\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.52 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       299\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16499.19 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.5s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.62s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.38s of the 4.38s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.22s of the 4.22s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.06s of the 4.06s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.97419e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 1926. Best iteration is:\n",
      "\t[1804]\tvalid_set's l2: 4.94561e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.16s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.38s of the -0.15s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.19s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_93\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 5s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v4_20/model_94\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.09 GB / 21.50 GB (74.8%)\n",
      "Disk Space Avail:   847.47 GB / 1006.85 GB (84.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       300\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16495.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #94\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.6s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.58s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 4.42s of the 4.42s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 4.27s of the 4.27s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 4.12s of the 4.11s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 4.99793e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 0.25s of the 0.25s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\tRan out of time, early stopping on iteration 23. Best iteration is:\n",
      "\t[23]\tvalid_set's l2: 0.000251772\n",
      "\t-0.0003\t = Validation score   (-mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 4.42s of the -0.09s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 1.0}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 5.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v4_20/model_94\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1h 7min 3s, sys: 13.4 s, total: 1h 7min 17s\n",
      "Wall time: 8min 11s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        #model_path = f'./autogluon_models/model_{i}'\n",
    "        model_path = f'../net95/autogluon_models_net95v4_20/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "fa1c8018-fe09-42e0-9687-f7565b57cb32",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_0/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_1/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_2/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_3/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_4/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_5/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_6/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_7/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_8/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_9/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_10/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_11/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_12/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_13/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_14/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_15/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_16/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_17/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_18/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_19/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_20/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_21/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_22/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_23/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_24/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_25/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_26/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_27/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_28/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_29/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_30/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_31/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_32/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_33/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_34/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_35/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_36/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_37/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_38/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_39/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_40/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_41/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_42/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_43/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_44/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_45/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_46/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_47/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_48/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_49/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_50/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_51/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_52/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_53/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_54/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_55/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_56/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_57/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_58/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_59/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_60/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_61/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_62/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_63/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_64/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_65/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_66/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_67/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_68/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_69/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_70/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_71/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_72/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_73/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_74/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_75/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_76/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_77/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_78/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_79/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_80/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_81/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_82/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_83/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_84/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_85/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_86/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_87/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_88/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_89/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_90/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_91/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_92/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_93/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v4_20/model_94/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n",
      "Model 18 loaded\n",
      "Model 19 loaded\n",
      "Model 20 loaded\n",
      "Model 21 loaded\n",
      "Model 22 loaded\n",
      "Model 23 loaded\n",
      "Model 24 loaded\n",
      "Model 25 loaded\n",
      "Model 26 loaded\n",
      "Model 27 loaded\n",
      "Model 28 loaded\n",
      "Model 29 loaded\n",
      "Model 30 loaded\n",
      "Model 31 loaded\n",
      "Model 32 loaded\n",
      "Model 33 loaded\n",
      "Model 34 loaded\n",
      "Model 35 loaded\n",
      "Model 36 loaded\n",
      "Model 37 loaded\n",
      "Model 38 loaded\n",
      "Model 39 loaded\n",
      "Model 40 loaded\n",
      "Model 41 loaded\n",
      "Model 42 loaded\n",
      "Model 43 loaded\n",
      "Model 44 loaded\n",
      "Model 45 loaded\n",
      "Model 46 loaded\n",
      "Model 47 loaded\n",
      "Model 48 loaded\n",
      "Model 49 loaded\n",
      "Model 50 loaded\n",
      "Model 51 loaded\n",
      "Model 52 loaded\n",
      "Model 53 loaded\n",
      "Model 54 loaded\n",
      "Model 55 loaded\n",
      "Model 56 loaded\n",
      "Model 57 loaded\n",
      "Model 58 loaded\n",
      "Model 59 loaded\n",
      "Model 60 loaded\n",
      "Model 61 loaded\n",
      "Model 62 loaded\n",
      "Model 63 loaded\n",
      "Model 64 loaded\n",
      "Model 65 loaded\n",
      "Model 66 loaded\n",
      "Model 67 loaded\n",
      "Model 68 loaded\n",
      "Model 69 loaded\n",
      "Model 70 loaded\n",
      "Model 71 loaded\n",
      "Model 72 loaded\n",
      "Model 73 loaded\n",
      "Model 74 loaded\n",
      "Model 75 loaded\n",
      "Model 76 loaded\n",
      "Model 77 loaded\n",
      "Model 78 loaded\n",
      "Model 79 loaded\n",
      "Model 80 loaded\n",
      "Model 81 loaded\n",
      "Model 82 loaded\n",
      "Model 83 loaded\n",
      "Model 84 loaded\n",
      "Model 85 loaded\n",
      "Model 86 loaded\n",
      "Model 87 loaded\n",
      "Model 88 loaded\n",
      "Model 89 loaded\n",
      "Model 90 loaded\n",
      "Model 91 loaded\n",
      "Model 92 loaded\n",
      "Model 93 loaded\n",
      "Model 94 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'../net95/autogluon_models_net95v4_20/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9457b0c4-a960-46fa-900f-1cf9a311fa0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "['std: 0.0013944992684399036', 'std: 0.0014564051540449765', 'std: 0.0013796591196219784', 'std: 0.0018137840804265544', 'std: 0.0015560371140055955']\n"
     ]
    }
   ],
   "source": [
    "from net95.scenarios2 import get_data_by_scenario_and_case\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    print(f'SCENARIO {scenario}, CASE 1 VALIDATION')\n",
    "    s1_c1_data = get_data_by_scenario_and_case(scenario, 1, net_name='net95v4')\n",
    "    x = s1_c1_data[0]\n",
    "    x_hat = s1_c1_data[1]\n",
    "    y_all = s1_c1_data[2]\n",
    "    y_hat_all = s1_c1_data[3]\n",
    "    \n",
    "    estim = []\n",
    "    for i in range(num_output):\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        \n",
    "        predictor = models[i]\n",
    "        test_x = x_hat\n",
    "        test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "        test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "        \n",
    "        \n",
    "        preds = predictor.predict(test)\n",
    "        \n",
    "        estim.append(preds[0])\n",
    "        \n",
    "    pred = np.asarray(estim)\n",
    "    std_results.append(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184a92a-313c-4f44-b27a-965952ebc557",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3119bc-edb3-4a60-9aea-4f9cbd6f594c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
