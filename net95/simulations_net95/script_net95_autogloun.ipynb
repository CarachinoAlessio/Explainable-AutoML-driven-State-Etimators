{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-18T11:20:27.208228Z",
     "start_time": "2024-05-18T11:20:26.123421Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import os\n",
    "import time\n",
    "from autogluon.tabular import TabularDataset, TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02c91eed-365d-43a6-a7af-5b1686b2b271",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/aless/progetti/ML-techniques-for-State-Estimation/net95'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8003b9514dd6505b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:39:13.619610400Z",
     "start_time": "2024-02-23T15:39:13.585967500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net95/net_95_v1/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net95/net_95_v1/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "815b96bd2fa485ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T15:39:15.440864200Z",
     "start_time": "2024-02-23T15:39:15.429397500Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "num_input = 206\n",
    "num_output = 95\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1e1333565ba3f6b",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_0\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.48 GB / 21.50 GB (76.6%)\n",
      "Disk Space Avail:   874.38 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       206\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16873.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.19s of the 19.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.02s of the 19.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.86s of the 18.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.94799e-06\n",
      "[2000]\tvalid_set's l2: 2.93101e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.46s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 12.35s of the 12.35s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.22738e-06\n",
      "[2000]\tvalid_set's l2: 3.20761e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 5.69s of the 5.69s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.15s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.19s of the -36.07s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.615, 'LightGBM': 0.385}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.12s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_0\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_1\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   874.38 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       207\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16839.57 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.88s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.12s of the 19.12s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 18.96s of the 18.95s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.8s of the 18.79s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.93425e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 14.57s of the 14.57s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.24079e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 10.6s of the 10.59s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.39s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.12s of the -31.43s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.632, 'LightGBM': 0.368}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_1\")\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_20/model_2\"\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.33 GB / 21.50 GB (75.9%)\n",
      "Disk Space Avail:   874.32 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       208\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16743.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.18s of the 19.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.01s of the 19.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.85s of the 18.85s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.91337e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.63s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 15.15s of the 15.15s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.31493e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.53s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 9.55s of the 9.55s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.26s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.18s of the -30.56s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.652, 'LightGBM': 0.348}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.61s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.41 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   874.07 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       209\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16829.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.22s of the 19.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.05s of the 19.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.9s of the 18.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.33s of the 16.33s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.01386e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.25792e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 12.39s of the 12.39s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.65s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.22s of the -28.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.611, 'LightGBM': 0.389}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 48.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.41 GB / 21.50 GB (76.3%)\n",
      "Disk Space Avail:   873.72 GB / 1006.85 GB (86.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       210\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16813.68 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.83s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.17s of the 19.17s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.0s of the 19.0s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.84s of the 18.84s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85603e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.78s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 15.03s of the 15.03s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.21815e-06\n",
      "[2000]\tvalid_set's l2: 3.20241e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.32s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 6.67s of the 6.67s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.56s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.17s of the -33.78s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.83s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   873.36 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       211\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16827.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.81s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.19s of the 19.19s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.02s of the 19.02s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.86s of the 18.86s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.97221e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.75s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.09s of the 16.09s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.79s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 13.28s of the 13.27s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t41.64s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.19s of the -29.28s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.647, 'LightGBM': 0.353}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.33s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.45 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   873.01 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       212\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16847.18 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.86s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.14s of the 19.14s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 18.98s of the 18.98s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.82s of the 18.82s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.32s of the 16.32s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 3.05598e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 12.39s of the 12.39s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t38.82s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.14s of the -27.11s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.579, 'LightGBM': 0.421}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.17s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   872.66 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       213\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16838.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.82s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.18s of the 19.18s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.01s of the 19.01s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.85s of the 18.85s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.89051e-06\n",
      "[2000]\tvalid_set's l2: 2.86765e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.76s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 14.04s of the 14.04s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 10.96s of the 10.96s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.63s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.18s of the -30.0s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.588, 'LightGBM': 0.412}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   872.30 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       214\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16841.21 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.22s of the 19.22s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.06s of the 19.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.09s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.88s of the 18.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87994e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.12s of the 16.12s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 14.58s of the 14.58s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.69s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.22s of the -25.44s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.55, 'LightGBMXT': 0.45}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.49s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   871.95 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       215\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16838.22 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.7s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.77s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.23s of the 19.23s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.07s of the 19.07s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.9s of the 18.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.87418e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 14.73s of the 14.73s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 13.25s of the 13.25s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.48s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.23s of the -26.51s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.522, 'LightGBMXT': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   871.60 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       216\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16830.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.2s of the 19.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.03s of the 19.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.87s of the 18.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.64s of the 16.64s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 15.07s of the 15.07s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t38.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.2s of the -24.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBM': 0.6, 'LightGBMXT': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.1s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   871.25 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       217\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16831.92 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.21s of the 19.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.09s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.04s of the 19.04s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.88s of the 18.88s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.77s of the 16.77s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 14.94s of the 14.94s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.65s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.21s of the -25.04s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.522, 'LightGBM': 0.478}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.5%)\n",
      "Disk Space Avail:   870.90 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       218\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16832.06 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.79s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.21s of the 19.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.05s of the 19.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.9s of the 18.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 16.86s of the 16.86s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 15.15s of the 15.15s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t40.54s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.21s of the -25.73s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.6, 'LightGBM': 0.4}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.78s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   870.55 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       219\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16824.63 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.8s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.2s of the 19.2s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.03s of the 19.03s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.08s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.87s of the 18.87s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 17.0s of the 17.0s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.63s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 15.36s of the 15.36s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t39.57s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.2s of the -24.5s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.565, 'LightGBM': 0.435}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.55s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.44 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   870.20 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       220\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16864.89 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 206 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.8s = Fit runtime\n",
      "\t206 features in original data used to generate 206 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.78s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 19.22s of the 19.21s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 19.05s of the 19.05s of remaining time.\n",
      "\t-0.0002\t = Validation score   (-mean_squared_error)\n",
      "\t0.07s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 18.9s of the 18.9s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.85178e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 15.6s of the 15.6s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 12.59s of the 12.59s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t42.23s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 19.22s of the -30.0s of remaining time.\n",
      "\tEnsemble Weights: {'LightGBMXT': 0.5, 'LightGBM': 0.5}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.05s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"../net95/autogluon_models_net95v1_20/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 20s\n",
      "AutoGluon will save models to \"../net95/autogluon_models_net95v1_20/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Fri Mar 29 23:14:13 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       16.43 GB / 21.50 GB (76.4%)\n",
      "Disk Space Avail:   869.85 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 206\n",
      "Label Column:       221\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    16864.32 MB\n",
      "\tTrain Data (Original)  Memory Usage: 21.12 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[0;32m<timed exec>:15\u001b[0m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/core/utils/decorators.py:31\u001b[0m, in \u001b[0;36munpack.<locals>._unpack_inner.<locals>._call\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_call\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     30\u001b[0m     gargs, gkwargs \u001b[38;5;241m=\u001b[39m g(\u001b[38;5;241m*\u001b[39mother_args, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mgkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/predictor/predictor.py:1136\u001b[0m, in \u001b[0;36mTabularPredictor.fit\u001b[0;34m(self, train_data, tuning_data, time_limit, presets, hyperparameters, feature_metadata, infer_limit, infer_limit_batch_size, fit_weighted_ensemble, fit_full_last_level_weighted_ensemble, full_weighted_ensemble_additionally, dynamic_stacking, calibrate_decision_threshold, num_cpus, num_gpus, **kwargs)\u001b[0m\n\u001b[1;32m   1133\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_stack_levels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m num_stack_levels\n\u001b[1;32m   1134\u001b[0m     ag_fit_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtime_limit\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m time_limit\n\u001b[0;32m-> 1136\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mag_post_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/predictor/predictor.py:1142\u001b[0m, in \u001b[0;36mTabularPredictor._fit\u001b[0;34m(self, ag_fit_kwargs, ag_post_fit_kwargs)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ag_fit_kwargs: \u001b[38;5;28mdict\u001b[39m, ag_post_fit_kwargs: \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave(silent\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)  \u001b[38;5;66;03m# Save predictor to disk to enable prediction and training after interrupt\u001b[39;00m\n\u001b[0;32m-> 1142\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_learner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mag_fit_kwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_set_post_fit_vars()\n\u001b[1;32m   1144\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_post_fit(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mag_post_fit_kwargs)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/learner/abstract_learner.py:159\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit\u001b[0;34m(self, X, X_val, **kwargs)\u001b[0m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAssertionError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLearner is already fit.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_validate_fit_input(X\u001b[38;5;241m=\u001b[39mX, X_val\u001b[38;5;241m=\u001b[39mX_val, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 159\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/learner/default_learner.py:93\u001b[0m, in \u001b[0;36mDefaultLearner._fit\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds, num_bag_sets, time_limit, infer_limit, infer_limit_batch_size, verbosity, **trainer_fit_kwargs)\u001b[0m\n\u001b[1;32m     91\u001b[0m X_og \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m infer_limit_batch_size \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m     92\u001b[0m logger\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreprocessing data ...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 93\u001b[0m X, y, X_val, y_val, X_unlabeled, holdout_frac, num_bag_folds, groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgeneral_data_processing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_unlabeled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mholdout_frac\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bag_folds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_og \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     95\u001b[0m     infer_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_infer_limit(X\u001b[38;5;241m=\u001b[39mX_og, infer_limit_batch_size\u001b[38;5;241m=\u001b[39minfer_limit_batch_size, infer_limit\u001b[38;5;241m=\u001b[39minfer_limit)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/learner/default_learner.py:317\u001b[0m, in \u001b[0;36mDefaultLearner.general_data_processing\u001b[0;34m(self, X, X_val, X_unlabeled, holdout_frac, num_bag_folds)\u001b[0m\n\u001b[1;32m    315\u001b[0m         y_unlabeled \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(np\u001b[38;5;241m.\u001b[39mnan, index\u001b[38;5;241m=\u001b[39mX_unlabeled\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m    316\u001b[0m         y_super \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([y, y_unlabeled], ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 317\u001b[0m     X_super \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_super\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_super\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproblem_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlabel_cleaner\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproblem_type_transform\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meval_metric\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meval_metric\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    319\u001b[0m X \u001b[38;5;241m=\u001b[39m X_super\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;28mlen\u001b[39m(X))\u001b[38;5;241m.\u001b[39mset_index(X\u001b[38;5;241m.\u001b[39mindex)\n\u001b[1;32m    320\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m X_unlabeled \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/tabular/learner/abstract_learner.py:459\u001b[0m, in \u001b[0;36mAbstractTabularLearner.fit_transform_features\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m    457\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mignored_columns, errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    458\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m feature_generator \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_generators:\n\u001b[0;32m--> 459\u001b[0m     X \u001b[38;5;241m=\u001b[39m \u001b[43mfeature_generator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    460\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m X\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/pipeline.py:68\u001b[0m, in \u001b[0;36mPipelineFeatureGenerator.fit_transform\u001b[0;34m(self, X, y, feature_metadata_in, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: DataFrame, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, feature_metadata_in: FeatureMetadata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m---> 68\u001b[0m     X_out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_metadata_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_metadata_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compute_post_memory_usage(X_out)\n\u001b[1;32m     70\u001b[0m     \u001b[38;5;66;03m# TODO: Consider adding final check of validity/that features are reasonable.\u001b[39;00m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.fit_transform\u001b[0;34m(self, X, y, feature_metadata_in, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# TODO: Add option to return feature_metadata instead to avoid data copy\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m#  If so, consider adding validation step to check that X_out matches the feature metadata, error/warning if not\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m X_out, type_family_groups_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m type_map_raw \u001b[38;5;241m=\u001b[39m get_type_map_raw(X_out)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_metadata_before_post \u001b[38;5;241m=\u001b[39m FeatureMetadata(type_map_raw\u001b[38;5;241m=\u001b[39mtype_map_raw, type_group_map_special\u001b[38;5;241m=\u001b[39mtype_family_groups_special)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/pipeline.py:75\u001b[0m, in \u001b[0;36mPipelineFeatureGenerator._fit_transform\u001b[0;34m(self, X, y, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: DataFrame, y\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m---> 75\u001b[0m     X_out, type_group_map_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     76\u001b[0m     X_out, type_group_map_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit_transform_custom(X_out\u001b[38;5;241m=\u001b[39mX_out, type_group_map_special\u001b[38;5;241m=\u001b[39mtype_group_map_special, y\u001b[38;5;241m=\u001b[39my)\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m X_out, type_group_map_special\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/bulk.py:131\u001b[0m, in \u001b[0;36mBulkFeatureGenerator._fit_transform\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m    129\u001b[0m         generator\u001b[38;5;241m.\u001b[39mverbosity \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbosity\n\u001b[1;32m    130\u001b[0m     generator\u001b[38;5;241m.\u001b[39mset_log_prefix(log_prefix\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_prefix \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, prepend\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m--> 131\u001b[0m     feature_df_list\u001b[38;5;241m.\u001b[39mappend(\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_metadata_in\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfeature_metadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    132\u001b[0m     generator_group_valid\u001b[38;5;241m.\u001b[39mappend(generator)\n\u001b[1;32m    133\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/abstract.py:280\u001b[0m, in \u001b[0;36mAbstractFeatureGenerator.fit_transform\u001b[0;34m(self, X, y, feature_metadata_in, **kwargs)\u001b[0m\n\u001b[1;32m    276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pre_astype_generator\u001b[38;5;241m.\u001b[39mfit(X)\n\u001b[1;32m    278\u001b[0m \u001b[38;5;66;03m# TODO: Add option to return feature_metadata instead to avoid data copy\u001b[39;00m\n\u001b[1;32m    279\u001b[0m \u001b[38;5;66;03m#  If so, consider adding validation step to check that X_out matches the feature metadata, error/warning if not\u001b[39;00m\n\u001b[0;32m--> 280\u001b[0m X_out, type_family_groups_special \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeatures_in\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    282\u001b[0m type_map_raw \u001b[38;5;241m=\u001b[39m get_type_map_raw(X_out)\n\u001b[1;32m    283\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_feature_metadata_before_post \u001b[38;5;241m=\u001b[39m FeatureMetadata(type_map_raw\u001b[38;5;241m=\u001b[39mtype_map_raw, type_group_map_special\u001b[38;5;241m=\u001b[39mtype_family_groups_special)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/drop_duplicates.py:44\u001b[0m, in \u001b[0;36mDropDuplicatesFeatureGenerator._fit_transform\u001b[0;34m(self, X, **kwargs)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit_transform\u001b[39m(\u001b[38;5;28mself\u001b[39m, X: DataFrame, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m (DataFrame, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m     43\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_size_init \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(X) \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msample_size_init:\n\u001b[0;32m---> 44\u001b[0m         features_to_check \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_duplicate_features\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_metadata_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample_size_init\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     45\u001b[0m         X_candidates \u001b[38;5;241m=\u001b[39m X[features_to_check]\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/drop_duplicates.py:72\u001b[0m, in \u001b[0;36mDropDuplicatesFeatureGenerator._drop_duplicate_features\u001b[0;34m(cls, X, feature_metadata_in, keep, sample_size)\u001b[0m\n\u001b[1;32m     70\u001b[0m features_to_check_numeric \u001b[38;5;241m=\u001b[39m [feature \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features_to_check_numeric \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m X_columns]\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m features_to_check_numeric:\n\u001b[0;32m---> 72\u001b[0m     features_to_remove \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mcls\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_duplicate_features_numeric\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeatures_to_check_numeric\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m     X \u001b[38;5;241m=\u001b[39m X\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39mfeatures_to_check_numeric)\n\u001b[1;32m     75\u001b[0m X_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/autogluon/features/generators/drop_duplicates.py:106\u001b[0m, in \u001b[0;36mDropDuplicatesFeatureGenerator._drop_duplicate_features_numeric\u001b[0;34m(cls, X, keep)\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(feature_sum_map[key]) \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    105\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m--> 106\u001b[0m     features_to_keep \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(\u001b[43mX\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfeature_sum_map\u001b[49m\u001b[43m[\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop_duplicates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mT\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    107\u001b[0m     features_to_remove \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m [feature \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m feature_sum_map[key] \u001b[38;5;28;01mif\u001b[39;00m feature \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m features_to_keep]\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m features_to_remove\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:6522\u001b[0m, in \u001b[0;36mDataFrame.drop_duplicates\u001b[0;34m(self, subset, keep, inplace, ignore_index)\u001b[0m\n\u001b[1;32m   6519\u001b[0m inplace \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(inplace, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minplace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6520\u001b[0m ignore_index \u001b[38;5;241m=\u001b[39m validate_bool_kwarg(ignore_index, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore_index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 6522\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m[\u001b[38;5;241m-\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mduplicated\u001b[49m\u001b[43m(\u001b[49m\u001b[43msubset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkeep\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep\u001b[49m\u001b[43m)\u001b[49m]\n\u001b[1;32m   6523\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ignore_index:\n\u001b[1;32m   6524\u001b[0m     result\u001b[38;5;241m.\u001b[39mindex \u001b[38;5;241m=\u001b[39m default_index(\u001b[38;5;28mlen\u001b[39m(result))\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:6662\u001b[0m, in \u001b[0;36mDataFrame.duplicated\u001b[0;34m(self, subset, keep)\u001b[0m\n\u001b[1;32m   6660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6661\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[0;32m-> 6662\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvals\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   6664\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(\n\u001b[1;32m   6665\u001b[0m         labels,\n\u001b[1;32m   6666\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6670\u001b[0m         xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   6671\u001b[0m     )\n\u001b[1;32m   6672\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced(duplicated(ids, keep), index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:6661\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m   6659\u001b[0m     result\u001b[38;5;241m.\u001b[39mname \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   6660\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 6661\u001b[0m     vals \u001b[38;5;241m=\u001b[39m (col\u001b[38;5;241m.\u001b[39mvalues \u001b[38;5;28;01mfor\u001b[39;00m name, col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m subset)\n\u001b[1;32m   6662\u001b[0m     labels, shape \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(f, vals)))\n\u001b[1;32m   6664\u001b[0m     ids \u001b[38;5;241m=\u001b[39m get_group_index(\n\u001b[1;32m   6665\u001b[0m         labels,\n\u001b[1;32m   6666\u001b[0m         \u001b[38;5;66;03m# error: Argument 1 to \"tuple\" has incompatible type \"List[_T]\";\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   6670\u001b[0m         xnull\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m   6671\u001b[0m     )\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:1349\u001b[0m, in \u001b[0;36mDataFrame.items\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mis_unique \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_item_cache\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m   1348\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m-> 1349\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m k, \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_item_cache\u001b[49m\u001b[43m(\u001b[49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1350\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns):\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:4256\u001b[0m, in \u001b[0;36mDataFrame._get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   4251\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m res \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   4252\u001b[0m     \u001b[38;5;66;03m# All places that call _get_item_cache have unique columns,\u001b[39;00m\n\u001b[1;32m   4253\u001b[0m     \u001b[38;5;66;03m#  pending resolution of GH#33047\u001b[39;00m\n\u001b[1;32m   4255\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mget_loc(item)\n\u001b[0;32m-> 4256\u001b[0m     res \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_ixs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4258\u001b[0m     cache[item] \u001b[38;5;241m=\u001b[39m res\n\u001b[1;32m   4260\u001b[0m     \u001b[38;5;66;03m# for a chain\u001b[39;00m\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:3667\u001b[0m, in \u001b[0;36mDataFrame._ixs\u001b[0;34m(self, i, axis)\u001b[0m\n\u001b[1;32m   3664\u001b[0m label \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns[i]\n\u001b[1;32m   3666\u001b[0m col_mgr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mgr\u001b[38;5;241m.\u001b[39miget(i)\n\u001b[0;32m-> 3667\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_box_col_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol_mgr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3669\u001b[0m \u001b[38;5;66;03m# this is a cached value, mark it so\u001b[39;00m\n\u001b[1;32m   3670\u001b[0m result\u001b[38;5;241m.\u001b[39m_set_as_cached(label, \u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/frame.py:4235\u001b[0m, in \u001b[0;36mDataFrame._box_col_values\u001b[0;34m(self, values, loc)\u001b[0m\n\u001b[1;32m   4233\u001b[0m klass \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_constructor_sliced\n\u001b[1;32m   4234\u001b[0m \u001b[38;5;66;03m# We get index=self.index bc values is a SingleDataManager\u001b[39;00m\n\u001b[0;32m-> 4235\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mklass\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfastpath\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m__finalize__(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/pandas/core/series.py:379\u001b[0m, in \u001b[0;36mSeries.__init__\u001b[0;34m(self, data, index, dtype, name, copy, fastpath)\u001b[0m\n\u001b[1;32m    368\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m    369\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    370\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    375\u001b[0m     fastpath: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[1;32m    376\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    377\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[1;32m    378\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(data, (SingleBlockManager, SingleArrayManager))\n\u001b[0;32m--> 379\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\n\u001b[1;32m    380\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    381\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m copy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    382\u001b[0m     ):\n\u001b[1;32m    383\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    384\u001b[0m             data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mcopy(deep\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "%%time\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        #model_path = f'./autogluon_models/model_{i}'\n",
    "        model_path = f'../net95/autogluon_models_net95v1_20/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc57d235ae0144b6",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "'''\n",
    "test_predictions = []\n",
    "for i in range(num_output):\n",
    "    columns_names = in_columns + [out_columns[i]]\n",
    "    x = in_columns\n",
    "    y = out_columns[i]\n",
    "    \n",
    "    test = np.hstack((test_x, test_y[:, i].reshape(-1, 1)))\n",
    "    test = h2o.H2OFrame(test, column_names=columns_names)\n",
    "    \n",
    "    model_path = f'./autogluon_models/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    \n",
    "    aml = h2o.load_model(f'{model_path}/{model_filename}')\n",
    "    try:\n",
    "        preds = aml.leader.predict(test)\n",
    "    except:\n",
    "        preds = aml.predict(test)\n",
    "    test_predictions.append(preds['predict'])\n",
    "    perf = aml.model_performance(test)\n",
    "    print(f\"MSE for model {i}: {perf._metric_json['MSE']}\")\n",
    "    print('---------------------------------')\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9685735c5423a2d7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T16:05:19.379751500Z",
     "start_time": "2024-02-23T16:05:19.343186800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_0/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_1/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_2/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_3/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_4/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_5/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_6/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_7/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_8/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_9/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_10/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_11/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_12/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_13/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_14/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_15/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_16/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_17/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_18/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_19/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_20/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_21/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_22/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_23/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_24/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_25/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_26/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_27/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_28/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_29/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_30/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_31/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_32/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_33/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_34/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_35/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_36/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_37/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_38/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_39/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_40/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_41/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_42/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_43/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_44/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_45/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_46/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_47/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_48/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_49/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_50/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_51/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_52/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_53/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_54/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_55/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_56/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_57/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_58/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_59/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_60/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_61/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_62/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_63/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_64/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_65/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_66/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_67/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_68/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_69/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_70/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_71/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_72/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_73/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_74/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_75/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_76/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_77/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_78/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_79/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_80/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_81/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_82/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_83/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_84/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_85/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_86/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_87/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_88/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_89/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_90/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n",
      "Model 18 loaded\n",
      "Model 19 loaded\n",
      "Model 20 loaded\n",
      "Model 21 loaded\n",
      "Model 22 loaded\n",
      "Model 23 loaded\n",
      "Model 24 loaded\n",
      "Model 25 loaded\n",
      "Model 26 loaded\n",
      "Model 27 loaded\n",
      "Model 28 loaded\n",
      "Model 29 loaded\n",
      "Model 30 loaded\n",
      "Model 31 loaded\n",
      "Model 32 loaded\n",
      "Model 33 loaded\n",
      "Model 34 loaded\n",
      "Model 35 loaded\n",
      "Model 36 loaded\n",
      "Model 37 loaded\n",
      "Model 38 loaded\n",
      "Model 39 loaded\n",
      "Model 40 loaded\n",
      "Model 41 loaded\n",
      "Model 42 loaded\n",
      "Model 43 loaded\n",
      "Model 44 loaded\n",
      "Model 45 loaded\n",
      "Model 46 loaded\n",
      "Model 47 loaded\n",
      "Model 48 loaded\n",
      "Model 49 loaded\n",
      "Model 50 loaded\n",
      "Model 51 loaded\n",
      "Model 52 loaded\n",
      "Model 53 loaded\n",
      "Model 54 loaded\n",
      "Model 55 loaded\n",
      "Model 56 loaded\n",
      "Model 57 loaded\n",
      "Model 58 loaded\n",
      "Model 59 loaded\n",
      "Model 60 loaded\n",
      "Model 61 loaded\n",
      "Model 62 loaded\n",
      "Model 63 loaded\n",
      "Model 64 loaded\n",
      "Model 65 loaded\n",
      "Model 66 loaded\n",
      "Model 67 loaded\n",
      "Model 68 loaded\n",
      "Model 69 loaded\n",
      "Model 70 loaded\n",
      "Model 71 loaded\n",
      "Model 72 loaded\n",
      "Model 73 loaded\n",
      "Model 74 loaded\n",
      "Model 75 loaded\n",
      "Model 76 loaded\n",
      "Model 77 loaded\n",
      "Model 78 loaded\n",
      "Model 79 loaded\n",
      "Model 80 loaded\n",
      "Model 81 loaded\n",
      "Model 82 loaded\n",
      "Model 83 loaded\n",
      "Model 84 loaded\n",
      "Model 85 loaded\n",
      "Model 86 loaded\n",
      "Model 87 loaded\n",
      "Model 88 loaded\n",
      "Model 89 loaded\n",
      "Model 90 loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_91/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_92/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_93/\"\n",
      "Warning: path already exists! This predictor may overwrite an existing predictor! path=\"../net95/autogluon_models_net95v1_60/model_94/\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 91 loaded\n",
      "Model 92 loaded\n",
      "Model 93 loaded\n",
      "Model 94 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'../net95/autogluon_models_net95v1_60/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c91c44008cd32294",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T16:09:14.938395Z",
     "start_time": "2024-02-23T16:08:39.688458300Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "['std: 0.0036916697383997375', 'std: 0.006825397869225171', 'std: 0.010664077238477123', 'std: 0.006565758425594769', 'std: 0.007180874790993515']\n"
     ]
    }
   ],
   "source": [
    "from net95.scenarios2 import get_data_by_scenario_and_case\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    print(f'SCENARIO {scenario}, CASE 1 VALIDATION')\n",
    "    s1_c1_data = get_data_by_scenario_and_case(scenario, 1)\n",
    "    x = s1_c1_data[0]\n",
    "    x_hat = s1_c1_data[1]\n",
    "    y_all = s1_c1_data[2]\n",
    "    y_hat_all = s1_c1_data[3]\n",
    "    \n",
    "    estim = []\n",
    "    for i in range(num_output):\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        \n",
    "        predictor = models[i]\n",
    "        test_x = x_hat\n",
    "        test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "        test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "        \n",
    "        \n",
    "        preds = predictor.predict(test)\n",
    "        \n",
    "        estim.append(preds[0])\n",
    "        \n",
    "    pred = np.asarray(estim)\n",
    "    std_results.append(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bf8dba40a9631851",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-02-23T14:05:34.810183300Z",
     "start_time": "2024-02-23T14:05:34.805170800Z"
    },
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std: 0.008439554785042516\n"
     ]
    }
   ],
   "source": [
    "pred = np.asarray(estim)\n",
    "print(f'std: {np.sqrt(np.mean(np.square(y_all - pred)))}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "621a0939a1bd162f",
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       11.07 GB / 15.49 GB (71.4%)\n",
      "Disk Space Avail:   897.11 GB / 1006.85 GB (89.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       53\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    11325.87 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.2s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.17s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.83s of the 59.83s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.59s of the 59.59s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.54s of the 59.53s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.29408e-06\n",
      "[2000]\tvalid_set's l2: 2.26518e-06\n",
      "[3000]\tvalid_set's l2: 2.26428e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.93s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 56.55s of the 56.55s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.64086e-06\n",
      "[2000]\tvalid_set's l2: 2.61716e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 54.19s of the 54.19s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.92s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 42.9s of the 42.9s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t13.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 29.56s of the 29.56s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 27.43s of the 27.43s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.21s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 17.19s of the 17.19s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:08:40] WARNING: /workspace/src/common/error_msg.cc:45: `gpu_id` is deprecated since2.0.0, use `device` instead. E.g. device=cpu/cuda/cuda:0\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:08:40] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:08:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:08:42] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
      "Potential solutions:\n",
      "- Use a data structure that matches the device ordinal in the booster.\n",
      "- Set the device for booster before call to inplace_predict.\n",
      "\n",
      "This warning will only be shown once.\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 15.18s of the 15.18s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 8.0s of the 8.0s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83994e-06\n",
      "[2000]\tvalid_set's l2: 2.83039e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tRan out of time, early stopping on iteration 2084. Best iteration is:\n",
      "\t[2081]\tvalid_set's l2: 2.83025e-06\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.17s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.83s of the -0.31s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.545, 'LightGBMXT': 0.227, 'CatBoost': 0.182, 'NeuralNetTorch': 0.045}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 60.37s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.98 GB / 15.49 GB (64.4%)\n",
      "Disk Space Avail:   896.45 GB / 1006.85 GB (89.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       54\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    10227.97 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.81s of the 59.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.76s of the 59.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.11407e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.39s of the 58.39s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.79s of the 57.78s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 47.33s of the 47.33s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t14.89s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 32.43s of the 32.43s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 30.35s of the 30.35s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 21.62s of the 21.62s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:09:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:09:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 20.35s of the 20.35s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.11s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.23s of the 13.23s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 10.85s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.44, 'LightGBMXT': 0.24, 'CatBoost': 0.12, 'ExtraTreesMSE': 0.12, 'LightGBM': 0.04, 'NeuralNetTorch': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.21s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.61 GB / 15.49 GB (62.0%)\n",
      "Disk Space Avail:   895.81 GB / 1006.85 GB (89.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       55\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9842.04 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.86s of the 59.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.8s of the 59.8s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.76s of the 59.75s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.13707e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.22s of the 58.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.63s of the 57.63s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.23s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 47.11s of the 47.11s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t11.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 35.2s of the 35.2s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 33.14s of the 33.14s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.67s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 24.45s of the 24.45s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:10:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:10:24] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.18s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 23.26s of the 23.26s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 16.77s of the 16.77s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.25s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.86s of the 14.48s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.417, 'LightGBMXT': 0.208, 'CatBoost': 0.083, 'ExtraTreesMSE': 0.083, 'XGBoost': 0.083, 'LightGBMLarge': 0.083, 'NeuralNetTorch': 0.042}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 45.58s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.59 GB / 15.49 GB (61.9%)\n",
      "Disk Space Avail:   895.18 GB / 1006.85 GB (88.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       56\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9827.93 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.87s of the 59.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.81s of the 59.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.76s of the 59.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.22365e-06\n",
      "[2000]\tvalid_set's l2: 2.20834e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.47s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.24s of the 57.24s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_set's l2: 2.21296e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.7s of the 56.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.61s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 45.78s of the 45.78s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t19.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 26.2s of the 26.2s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 24.09s of the 24.08s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 16.53s of the 16.52s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:11:16] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:11:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 15.21s of the 15.21s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.73s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 9.47s of the 9.47s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.87s of the 5.91s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.471, 'LightGBMXT': 0.118, 'CatBoost': 0.118, 'LightGBM': 0.059, 'ExtraTreesMSE': 0.059, 'XGBoost': 0.059, 'NeuralNetTorch': 0.059, 'LightGBMLarge': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.58 GB / 15.49 GB (61.9%)\n",
      "Disk Space Avail:   894.54 GB / 1006.85 GB (88.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       57\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9820.13 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.82s of the 59.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.77s of the 59.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.25683e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.46s of the 58.46s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.75s of the 57.75s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.46s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.98s of the 46.98s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t20.49s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 26.48s of the 26.48s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 24.46s of the 24.46s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.94s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 15.49s of the 15.49s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:12:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:12:13] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 14.1s of the 14.1s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 8.76s of the 8.76s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 5.82s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.143, 'CatBoost': 0.143, 'XGBoost': 0.071, 'NeuralNetTorch': 0.071, 'LightGBMLarge': 0.071}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.25s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.53 GB / 15.49 GB (61.5%)\n",
      "Disk Space Avail:   893.90 GB / 1006.85 GB (88.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       58\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9760.86 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16082e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.37s of the 58.37s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.62s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.74s of the 57.73s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.6s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.83s of the 46.83s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t15.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 31.47s of the 31.46s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 29.38s of the 29.38s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 20.67s of the 20.67s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:13:01] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:13:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 19.11s of the 19.11s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.48s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 13.62s of the 13.62s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 10.88s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.208, 'CatBoost': 0.083, 'XGBoost': 0.083, 'LightGBMLarge': 0.083, 'NeuralNetTorch': 0.042}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.18s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.52 GB / 15.49 GB (61.4%)\n",
      "Disk Space Avail:   893.26 GB / 1006.85 GB (88.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       59\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9751.47 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.86s of the 59.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.81s of the 59.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.76s of the 59.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.25978e-06\n",
      "[2000]\tvalid_set's l2: 2.24181e-06\n",
      "[3000]\tvalid_set's l2: 2.23863e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.17s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 56.53s of the 56.53s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000]\tvalid_set's l2: 2.24296e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 55.66s of the 55.66s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.03s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 45.33s of the 45.33s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t15.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 29.43s of the 29.43s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 27.36s of the 27.35s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.98s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 18.35s of the 18.35s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:13:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:13:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 17.05s of the 17.05s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.99s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 11.05s of the 11.05s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.87s of the 8.05s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.545, 'LightGBMXT': 0.136, 'CatBoost': 0.091, 'XGBoost': 0.091, 'LightGBMLarge': 0.091, 'NeuralNetTorch': 0.045}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 52.0s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.52 GB / 15.49 GB (61.5%)\n",
      "Disk Space Avail:   892.61 GB / 1006.85 GB (88.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       60\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9756.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.20046e-06\n",
      "[2000]\tvalid_set's l2: 2.19046e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.2s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.52s of the 57.52s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.59s of the 56.59s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.25s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.05s of the 46.05s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t19.68s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 26.37s of the 26.37s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 24.3s of the 24.3s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t9.05s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 15.22s of the 15.22s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:14:47] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:14:49] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 13.71s of the 13.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.52s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 8.17s of the 8.17s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 5.57s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.524, 'LightGBMXT': 0.19, 'CatBoost': 0.095, 'ExtraTreesMSE': 0.048, 'XGBoost': 0.048, 'NeuralNetTorch': 0.048, 'LightGBMLarge': 0.048}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.48s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.52 GB / 15.49 GB (61.4%)\n",
      "Disk Space Avail:   891.97 GB / 1006.85 GB (88.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       61\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9750.20 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.16s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.84s of the 59.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.73s of the 59.73s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.91s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.79s of the 58.79s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.87s of the 57.87s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.53s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 47.03s of the 47.03s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t20.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 27.03s of the 27.03s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 24.94s of the 24.94s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 16.22s of the 16.22s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:15:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:15:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 14.71s of the 14.71s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.33s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 9.37s of the 9.37s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.84s of the 6.63s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.529, 'LightGBMXT': 0.235, 'LightGBM': 0.059, 'CatBoost': 0.059, 'ExtraTreesMSE': 0.059, 'NeuralNetTorch': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.43s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.48 GB / 15.49 GB (61.2%)\n",
      "Disk Space Avail:   891.34 GB / 1006.85 GB (88.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       62\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9708.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.87s of the 59.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.82s of the 59.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.77s of the 59.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16993e-06\n",
      "[2000]\tvalid_set's l2: 2.14566e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.31s of the 57.31s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.87s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.42s of the 56.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.35s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 45.82s of the 45.82s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t20.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 25.21s of the 25.21s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 23.19s of the 23.19s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t9.0s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 14.17s of the 14.17s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:16:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:16:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 12.77s of the 12.77s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 6.99s of the 6.98s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.97s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.87s of the 3.97s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.222, 'CatBoost': 0.111, 'ExtraTreesMSE': 0.056, 'XGBoost': 0.056, 'NeuralNetTorch': 0.056}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.09s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.41 GB / 15.49 GB (60.7%)\n",
      "Disk Space Avail:   890.69 GB / 1006.85 GB (88.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       63\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9643.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.86s of the 59.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.79s of the 59.79s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.74s of the 59.74s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.17919e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.94s of the 57.94s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 2.16869e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15422e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.36s of the 56.36s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.01s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.09s of the 46.09s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t17.42s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 28.67s of the 28.67s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 26.61s of the 26.61s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 18.56s of the 18.56s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:17:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:17:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 17.02s of the 17.02s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 11.77s of the 11.76s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.86s of the 9.29s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.188, 'CatBoost': 0.125, 'LightGBM': 0.062, 'NeuralNetTorch': 0.062, 'LightGBMLarge': 0.062}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.77s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.43 GB / 15.49 GB (60.9%)\n",
      "Disk Space Avail:   890.05 GB / 1006.85 GB (88.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       64\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9661.54 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16854e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.14s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.62s of the 58.62s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.8s of the 57.8s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.5s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 47.04s of the 47.04s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t20.19s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 26.84s of the 26.84s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 24.78s of the 24.78s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 15.99s of the 15.99s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:18:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:18:23] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.38s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 14.59s of the 14.59s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.62s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 8.96s of the 8.96s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.3s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 6.64s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.2, 'CatBoost': 0.2, 'NeuralNetTorch': 0.05, 'LightGBMLarge': 0.05}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.42s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.43 GB / 15.49 GB (60.8%)\n",
      "Disk Space Avail:   889.42 GB / 1006.85 GB (88.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       65\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9657.01 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.89s of the 59.89s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.24408e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.47s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.28s of the 58.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.37s of the 57.37s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.2s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.91s of the 46.91s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t19.34s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 27.56s of the 27.56s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.81s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 25.45s of the 25.45s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 17.15s of the 17.15s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:19:14] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:19:15] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 15.56s of the 15.56s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 9.82s of the 9.82s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.89s of the 6.41s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.583, 'LightGBMXT': 0.125, 'LightGBM': 0.083, 'NeuralNetTorch': 0.083, 'CatBoost': 0.042, 'XGBoost': 0.042, 'LightGBMLarge': 0.042}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.65s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.41 GB / 15.49 GB (60.8%)\n",
      "Disk Space Avail:   888.78 GB / 1006.85 GB (88.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       66\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9644.91 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.14771e-06\n",
      "[2000]\tvalid_set's l2: 2.13956e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.96s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.78s of the 57.78s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.01s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.76s of the 56.76s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.07s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.42s of the 46.42s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t18.17s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 28.25s of the 28.25s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 26.23s of the 26.23s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.98s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 17.22s of the 17.22s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:20:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:20:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.71s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 15.49s of the 15.49s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 9.77s of the 9.77s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 6.23s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.571, 'LightGBMXT': 0.214, 'LightGBMLarge': 0.143, 'NeuralNetTorch': 0.071}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.83s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.41 GB / 15.49 GB (60.7%)\n",
      "Disk Space Avail:   888.13 GB / 1006.85 GB (88.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       67\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9643.25 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.82s of the 59.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.77s of the 59.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15724e-06\n",
      "[2000]\tvalid_set's l2: 2.13155e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.89s of the 57.89s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.31s of the 57.3s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.39s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.64s of the 46.64s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 36.27s of the 36.27s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 34.27s of the 34.27s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t9.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 25.18s of the 25.18s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:20:53] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:20:54] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 23.97s of the 23.97s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 18.16s of the 18.16s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 15.71s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.429, 'LightGBMXT': 0.19, 'LightGBMLarge': 0.19, 'CatBoost': 0.095, 'XGBoost': 0.048, 'NeuralNetTorch': 0.048}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.35s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.42 GB / 15.49 GB (60.8%)\n",
      "Disk Space Avail:   887.50 GB / 1006.85 GB (88.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       68\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9649.50 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.88s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.1898e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.44s of the 58.44s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.86s of the 57.86s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 47.36s of the 47.36s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t14.85s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 32.51s of the 32.51s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 30.47s of the 30.47s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.58s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 21.86s of the 21.86s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:21:41] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:21:42] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.24s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 20.61s of the 20.61s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 14.95s of the 14.95s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.42s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.88s of the 12.49s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.455, 'LightGBMXT': 0.136, 'CatBoost': 0.136, 'LightGBMLarge': 0.136, 'XGBoost': 0.091, 'NeuralNetTorch': 0.045}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.40 GB / 15.49 GB (60.7%)\n",
      "Disk Space Avail:   886.86 GB / 1006.85 GB (88.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       69\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9629.17 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.89s of the 59.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.83s of the 59.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.78s of the 59.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15538e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 58.03s of the 58.03s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 2.14383e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 57.45s of the 57.45s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.37s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.82s of the 46.82s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t17.5s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 29.31s of the 29.31s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 27.31s of the 27.3s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.83s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 19.44s of the 19.44s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:22:31] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:22:32] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 18.08s of the 18.07s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 12.3s of the 12.3s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.89s of the 9.21s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.44, 'LightGBMXT': 0.16, 'CatBoost': 0.16, 'XGBoost': 0.12, 'LightGBMLarge': 0.08, 'NeuralNetTorch': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.84s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 60s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_60/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.39 GB / 15.49 GB (60.6%)\n",
      "Disk Space Avail:   886.22 GB / 1006.85 GB (88.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       70\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9618.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.11s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 59.89s of the 59.89s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 59.84s of the 59.84s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 59.79s of the 59.79s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.19825e-06\n",
      "[2000]\tvalid_set's l2: 2.18201e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 57.44s of the 57.44s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.54s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 56.9s of the 56.9s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 46.51s of the 46.51s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t13.0s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 33.51s of the 33.51s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 31.46s of the 31.46s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.3s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 23.13s of the 23.13s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:23:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [11:23:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 21.8s of the 21.8s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.8s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 15.99s of the 15.99s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.8s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 59.89s of the 13.15s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.45, 'LightGBMXT': 0.15, 'CatBoost': 0.15, 'LightGBMLarge': 0.1, 'ExtraTreesMSE': 0.05, 'XGBoost': 0.05, 'NeuralNetTorch': 0.05}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 46.91s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_60/model_17\")\n"
     ]
    }
   ],
   "source": [
    "alt_x = np.load('./simulations_net18/net_18_v1/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net18/net_18_v1/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "num_input = 53\n",
    "num_output = 18\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]\n",
    "\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        model_path = f'./autogluon_models_net18v1_60/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=60)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3802cedb-d127-4e76-8bc4-8773609b1699",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'./autogluon_models_net95v1_60/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor.load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0f35a6b3-5c44-45d4-946b-0636600bd7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 1, CASE 2 VALIDATION\n",
      "SCENARIO 1, CASE 3 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 2 VALIDATION\n",
      "SCENARIO 2, CASE 3 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 2 VALIDATION\n",
      "SCENARIO 3, CASE 3 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 2 VALIDATION\n",
      "SCENARIO 4, CASE 3 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 2 VALIDATION\n",
      "SCENARIO 5, CASE 3 VALIDATION\n",
      "['rmse: 0.00752324455132659 mse: 5.6599208579065225e-05', 'rmse: 0.005233244846302057 mse: 2.7386851621347035e-05', 'rmse: 0.0033583632153721668 mse: 1.1278603486364877e-05', 'rmse: 0.00386517304813651 mse: 1.4939562692040878e-05', 'rmse: 0.0012791536215404386 mse: 1.6362339875000197e-06']\n"
     ]
    }
   ],
   "source": [
    "from net18.scenarios2 import get_data_by_scenario_and_case, report_preds_on_validation_files\n",
    "np.set_printoptions(suppress = True, formatter = {'float_kind':'{:0.6f}'.format})\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    for case in range(1, 4):\n",
    "        print(f'SCENARIO {scenario}, CASE {case} VALIDATION')\n",
    "        s1_c1_data = get_data_by_scenario_and_case(scenario, case)\n",
    "        x = s1_c1_data[0]\n",
    "        x_hat = s1_c1_data[1]\n",
    "        y_all = s1_c1_data[2]\n",
    "        y_hat_all = s1_c1_data[3]\n",
    "        \n",
    "        estim = []\n",
    "        for i in range(num_output):\n",
    "            columns_names = in_columns + [out_columns[i]]\n",
    "            x = in_columns\n",
    "            y = out_columns[i]\n",
    "            \n",
    "            predictor = models[i]\n",
    "            test_x = x_hat\n",
    "            test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "            test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "            \n",
    "            \n",
    "            preds = predictor.predict(test)\n",
    "            \n",
    "            estim.append(preds[0])\n",
    "            \n",
    "        pred = np.asarray(estim)\n",
    "        # report_preds_on_validation_files(pred, 8, 'autogloun', scenario, case=case)\n",
    "        if case == 1:\n",
    "            std_results.append(f'rmse: {np.sqrt(np.mean(np.square(y_all - pred)))} mse: {np.mean(np.square(y_all - pred))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea486df-fe3f-42e1-bf91-0d220e4c5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "alt_x = np.load('./simulations_net18/net_18_v1/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net18/net_18_v1/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "num_input = 53\n",
    "num_output = 18\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]\n",
    "\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        model_path = f'./autogluon_models_net18v1_600/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=600)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "33c64dfe-0a3b-4fef-a25b-3d219abd5929",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_0\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.33 GB / 15.49 GB (60.2%)\n",
      "Disk Space Avail:   872.68 GB / 1006.85 GB (86.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       53\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9559.46 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.8s of the 299.8s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.75s of the 299.75s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.29408e-06\n",
      "[2000]\tvalid_set's l2: 2.26518e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.69s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.01s of the 297.01s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_set's l2: 2.26428e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.64086e-06\n",
      "[2000]\tvalid_set's l2: 2.61716e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 294.68s of the 294.68s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.81s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 283.59s of the 283.59s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t22.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 261.18s of the 261.18s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.82s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 259.1s of the 259.1s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.34s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 250.74s of the 250.74s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:13:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:13:21] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 248.8s of the 248.8s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.65s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 242.15s of the 242.15s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.83994e-06\n",
      "[2000]\tvalid_set's l2: 2.83039e-06\n",
      "[3000]\tvalid_set's l2: 2.83003e-06\n",
      "[4000]\tvalid_set's l2: 2.82996e-06\n",
      "[5000]\tvalid_set's l2: 2.82997e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t19.84s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 222.04s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.52, 'CatBoost': 0.24, 'LightGBMXT': 0.2, 'NeuralNetTorch': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 78.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_0\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_1\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.19 GB / 15.49 GB (59.3%)\n",
      "Disk Space Avail:   871.99 GB / 1006.85 GB (86.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       54\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9419.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.85s of the 299.85s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.75s of the 299.74s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.11407e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.33s of the 298.33s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.64s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.68s of the 297.68s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 286.65s of the 286.65s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t13.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 272.7s of the 272.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 270.7s of the 270.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.03s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 262.64s of the 262.64s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:14:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:14:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.27s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 261.35s of the 261.35s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.92s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 254.42s of the 254.42s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.85s of the 252.03s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.44, 'LightGBMXT': 0.24, 'CatBoost': 0.12, 'ExtraTreesMSE': 0.12, 'LightGBM': 0.04, 'NeuralNetTorch': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 48.02s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_1\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_2\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.18 GB / 15.49 GB (59.3%)\n",
      "Disk Space Avail:   871.35 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       55\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9408.78 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.13707e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.23s of the 298.23s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.64s of the 297.64s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.14s of the 287.14s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t11.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 275.55s of the 275.55s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 273.52s of the 273.52s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.56s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 264.93s of the 264.93s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:15:11] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:15:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 263.75s of the 263.75s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 258.31s of the 258.31s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.29s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 256.0s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.417, 'LightGBMXT': 0.208, 'CatBoost': 0.083, 'ExtraTreesMSE': 0.083, 'XGBoost': 0.083, 'LightGBMLarge': 0.083, 'NeuralNetTorch': 0.042}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 44.06s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_2\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_3\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.19 GB / 15.49 GB (59.3%)\n",
      "Disk Space Avail:   870.72 GB / 1006.85 GB (86.5%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       56\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9420.84 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.76s of the 299.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.22365e-06\n",
      "[2000]\tvalid_set's l2: 2.20834e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.22s of the 297.22s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3000]\tvalid_set's l2: 2.21296e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.51s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.7s of the 296.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.57s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.86s of the 285.86s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t18.43s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 267.42s of the 267.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.85s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 265.31s of the 265.31s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.55s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 257.74s of the 257.74s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:16:02] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:16:03] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 256.5s of the 256.5s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 251.31s of the 251.31s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.19s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 249.09s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.471, 'LightGBMXT': 0.118, 'CatBoost': 0.118, 'LightGBM': 0.059, 'ExtraTreesMSE': 0.059, 'XGBoost': 0.059, 'NeuralNetTorch': 0.059, 'LightGBMLarge': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 50.96s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_3\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_4\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.19 GB / 15.49 GB (59.3%)\n",
      "Disk Space Avail:   870.08 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       57\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9414.65 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.25683e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.48s of the 298.48s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.77s of the 297.77s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.25s of the 287.25s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t21.75s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 265.49s of the 265.49s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 263.46s of the 263.46s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.88s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 255.56s of the 255.55s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:16:55] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:16:57] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.41s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 254.12s of the 254.12s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.25s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 248.87s of the 248.87s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.92s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 245.92s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.143, 'CatBoost': 0.143, 'XGBoost': 0.071, 'NeuralNetTorch': 0.071, 'LightGBMLarge': 0.071}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 54.14s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_4\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_5\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.18 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   869.44 GB / 1006.85 GB (86.4%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       58\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9401.96 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16082e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.32s of the 298.32s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.6s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.71s of the 297.71s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.47s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 286.97s of the 286.97s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t16.4s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 270.57s of the 270.57s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 268.57s of the 268.57s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.4s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 260.15s of the 260.15s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:17:45] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:17:46] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 258.7s of the 258.7s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.47s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 253.22s of the 253.22s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.7s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 250.49s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.208, 'CatBoost': 0.083, 'XGBoost': 0.083, 'LightGBMLarge': 0.083, 'NeuralNetTorch': 0.042}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 49.56s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_5\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_6\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.18 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   868.80 GB / 1006.85 GB (86.3%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       59\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9402.10 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.76s of the 299.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.25978e-06\n",
      "[2000]\tvalid_set's l2: 2.24181e-06\n",
      "[3000]\tvalid_set's l2: 2.23863e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.23s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 296.47s of the 296.47s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4000]\tvalid_set's l2: 2.24296e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.84s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 295.61s of the 295.61s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.24s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.1s of the 285.09s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t16.14s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 268.95s of the 268.95s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 266.9s of the 266.9s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.52s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 258.36s of the 258.36s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:18:36] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:18:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.28s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 257.06s of the 257.06s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.86s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 251.19s of the 251.19s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.95s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 248.2s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.545, 'LightGBMXT': 0.136, 'CatBoost': 0.091, 'XGBoost': 0.091, 'LightGBMLarge': 0.091, 'NeuralNetTorch': 0.045}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 51.85s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_6\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_7\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   868.16 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       60\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9397.69 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.20046e-06\n",
      "[2000]\tvalid_set's l2: 2.19046e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.33s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.4s of the 297.4s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.96s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.42s of the 296.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.18s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.95s of the 285.95s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t23.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 262.91s of the 262.91s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 260.83s of the 260.83s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.81s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 253.0s of the 253.0s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:19:34] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:19:35] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 251.43s of the 251.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t4.94s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 246.48s of the 246.48s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.6s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 243.84s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.524, 'LightGBMXT': 0.19, 'CatBoost': 0.095, 'ExtraTreesMSE': 0.048, 'XGBoost': 0.048, 'NeuralNetTorch': 0.048, 'LightGBMLarge': 0.048}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 56.22s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_7\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_8\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   867.51 GB / 1006.85 GB (86.2%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       61\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9395.59 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.77s of the 299.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.93s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.82s of the 298.82s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.9s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.91s of the 297.9s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.23s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.4s of the 287.4s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t21.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 266.04s of the 266.04s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 263.95s of the 263.95s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 255.84s of the 255.83s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:20:27] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:20:28] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.39s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 254.42s of the 254.42s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 249.11s of the 249.11s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.68s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 246.39s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.529, 'LightGBMXT': 0.235, 'LightGBM': 0.059, 'CatBoost': 0.059, 'ExtraTreesMSE': 0.059, 'NeuralNetTorch': 0.059}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 53.66s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_8\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_9\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   866.88 GB / 1006.85 GB (86.1%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       62\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9397.14 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16993e-06\n",
      "[2000]\tvalid_set's l2: 2.14566e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.45s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.28s of the 297.28s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.88s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.39s of the 296.39s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.27s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.86s of the 285.86s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t24.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 261.61s of the 261.61s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 259.55s of the 259.55s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 251.75s of the 251.75s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:21:25] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:21:26] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.43s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 250.31s of the 250.31s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.98s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 244.32s of the 244.32s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.04s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 241.24s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.222, 'CatBoost': 0.111, 'ExtraTreesMSE': 0.056, 'XGBoost': 0.056, 'NeuralNetTorch': 0.056}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 58.82s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_9\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_10\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   866.23 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       63\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9395.35 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.17919e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.92s of the 297.92s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 2.16869e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15422e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.54s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.36s of the 296.35s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.57s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.52s of the 285.52s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t16.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 268.95s of the 268.95s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.83s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 266.86s of the 266.86s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.83s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 258.0s of the 258.0s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:22:17] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:22:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 256.62s of the 256.62s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.18s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 250.44s of the 250.44s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.5s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 247.91s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.188, 'CatBoost': 0.125, 'LightGBM': 0.062, 'NeuralNetTorch': 0.062, 'LightGBMLarge': 0.062}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 52.15s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_10\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_11\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   865.59 GB / 1006.85 GB (86.0%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       64\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9396.61 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.16854e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.12s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.63s of the 298.63s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.78s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.83s of the 297.83s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.34s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.23s of the 287.23s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t19.7s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 267.53s of the 267.53s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.75s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 265.52s of the 265.52s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.53s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 256.97s of the 256.97s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:23:10] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:23:12] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.56s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 255.4s of the 255.4s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.21s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 250.18s of the 250.18s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.35s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 247.79s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.5, 'LightGBMXT': 0.2, 'CatBoost': 0.2, 'NeuralNetTorch': 0.05, 'LightGBMLarge': 0.05}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 52.26s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_11\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_12\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   864.96 GB / 1006.85 GB (85.9%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       65\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9391.27 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.83s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #12\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.78s of the 299.78s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.24408e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.48s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.27s of the 298.27s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.89s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.36s of the 297.36s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.06s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.04s of the 287.04s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t25.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 261.72s of the 261.72s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 259.68s of the 259.68s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 251.93s of the 251.93s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:24:08] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:24:09] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.62s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 250.26s of the 250.26s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.61s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 244.64s of the 244.64s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.84s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 240.75s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.565, 'LightGBMXT': 0.13, 'CatBoost': 0.087, 'NeuralNetTorch': 0.087, 'LightGBM': 0.043, 'XGBoost': 0.043, 'LightGBMLarge': 0.043}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 59.31s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_12\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_13\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   864.32 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       66\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9391.58 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #13\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.75s of the 299.75s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.14771e-06\n",
      "[2000]\tvalid_set's l2: 2.13956e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.09s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.62s of the 297.62s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.03s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.58s of the 296.57s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.33s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.98s of the 285.98s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t22.66s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 263.31s of the 263.31s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.79s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 261.24s of the 261.24s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.9s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 253.32s of the 253.32s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:25:06] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:25:07] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.73s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 251.57s of the 251.57s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.44s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 246.12s of the 246.12s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.57s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 242.51s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.571, 'LightGBMXT': 0.214, 'LightGBMLarge': 0.143, 'NeuralNetTorch': 0.071}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 57.54s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_13\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_14\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.17 GB / 15.49 GB (59.2%)\n",
      "Disk Space Avail:   863.67 GB / 1006.85 GB (85.8%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       67\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9390.44 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.14s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.86s of the 299.86s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #14\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.81s of the 299.81s of remaining time.\n",
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.76s of the 299.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15724e-06\n",
      "[2000]\tvalid_set's l2: 2.13155e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.68s of the 297.68s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.08s of the 297.08s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.73s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 286.08s of the 286.08s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.81s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 275.26s of the 275.26s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.87s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 273.13s of the 273.13s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t7.87s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 265.23s of the 265.23s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:25:51] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:25:52] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 264.04s of the 264.04s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.31s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 258.73s of the 258.73s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.44s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.86s of the 256.25s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.429, 'LightGBMXT': 0.19, 'LightGBMLarge': 0.19, 'CatBoost': 0.095, 'XGBoost': 0.048, 'NeuralNetTorch': 0.048}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 43.81s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_14\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_15\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.16 GB / 15.49 GB (59.1%)\n",
      "Disk Space Avail:   863.04 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       68\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9383.76 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #15\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.77s of the 299.76s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.1898e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 298.37s of the 298.37s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.57s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.79s of the 297.79s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.5s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 287.01s of the 287.01s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t14.28s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 272.72s of the 272.72s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.76s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 270.67s of the 270.67s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.21s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 262.44s of the 262.44s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:26:38] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:26:39] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.17s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 261.25s of the 261.25s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.77s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 255.47s of the 255.47s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.39s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 253.05s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.455, 'LightGBMXT': 0.136, 'CatBoost': 0.136, 'LightGBMLarge': 0.136, 'XGBoost': 0.091, 'NeuralNetTorch': 0.045}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 47.0s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_15\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_16\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.16 GB / 15.49 GB (59.1%)\n",
      "Disk Space Avail:   862.40 GB / 1006.85 GB (85.7%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       69\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9385.05 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.13s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.87s of the 299.87s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.82s of the 299.82s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #16\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.77s of the 299.77s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.15538e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.77s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.97s of the 297.97s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2000]\tvalid_set's l2: 2.14383e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.59s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 297.37s of the 297.37s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.64s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 286.44s of the 286.44s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t17.37s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 269.07s of the 269.07s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.78s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 267.01s of the 267.01s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.85s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 258.14s of the 258.14s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:27:29] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:27:30] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.31s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 256.8s of the 256.8s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t6.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 250.56s of the 250.56s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t3.09s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.87s of the 247.43s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.44, 'LightGBMXT': 0.16, 'CatBoost': 0.16, 'XGBoost': 0.12, 'LightGBMLarge': 0.08, 'NeuralNetTorch': 0.04}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 52.63s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_16\")\n",
      "Presets specified: ['medium_quality']\n",
      "Beginning AutoGluon training ... Time limit = 300s\n",
      "AutoGluon will save models to \"./autogluon_models_net18v1_300/model_17\"\n",
      "=================== System Info ===================\n",
      "AutoGluon Version:  1.1.0\n",
      "Python Version:     3.8.18\n",
      "Operating System:   Linux\n",
      "Platform Machine:   x86_64\n",
      "Platform Version:   #1 SMP Thu Jan 11 04:09:03 UTC 2024\n",
      "CPU Count:          20\n",
      "Memory Avail:       9.20 GB / 15.49 GB (59.4%)\n",
      "Disk Space Avail:   861.76 GB / 1006.85 GB (85.6%)\n",
      "===================================================\n",
      "Train Data Rows:    13440\n",
      "Train Data Columns: 53\n",
      "Label Column:       70\n",
      "Problem Type:       regression\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    9429.88 MB\n",
      "\tTrain Data (Original)  Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tStage 5 Generators:\n",
      "\t\tFitting DropDuplicatesFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', []) : 53 | ['0', '1', '2', '3', '4', ...]\n",
      "\t0.1s = Fit runtime\n",
      "\t53 features in original data used to generate 53 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 5.43 MB (0.1% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'mean_squared_error'\n",
      "\tThis metric's sign has been flipped to adhere to being higher_is_better. The metric score can be multiplied by -1 to get the metric value.\n",
      "\tTo change this, specify the eval_metric parameter of Predictor()\n",
      "Automatically generating train/validation split with holdout_frac=0.1, Train Rows: 12096, Val Rows: 1344\n",
      "User-specified model hyperparameters to be fit:\n",
      "{\n",
      "\t'NN_TORCH': {},\n",
      "\t'GBM': [{'extra_trees': True, 'ag_args': {'name_suffix': 'XT'}}, {}, 'GBMLarge'],\n",
      "\t'CAT': {},\n",
      "\t'XGB': {},\n",
      "\t'FASTAI': {},\n",
      "\t'RF': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'XT': [{'criterion': 'gini', 'ag_args': {'name_suffix': 'Gini', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'entropy', 'ag_args': {'name_suffix': 'Entr', 'problem_types': ['binary', 'multiclass']}}, {'criterion': 'squared_error', 'ag_args': {'name_suffix': 'MSE', 'problem_types': ['regression', 'quantile']}}],\n",
      "\t'KNN': [{'weights': 'uniform', 'ag_args': {'name_suffix': 'Unif'}}, {'weights': 'distance', 'ag_args': {'name_suffix': 'Dist'}}],\n",
      "}\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif ... Training model for up to 299.88s of the 299.88s of remaining time.\n",
      "\t-0.0007\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist ... Training model for up to 299.84s of the 299.83s of remaining time.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for output #17\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0006\t = Validation score   (-mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBMXT ... Training model for up to 299.79s of the 299.79s of remaining time.\n",
      "\tTraining LightGBMXT with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\tvalid_set's l2: 2.19825e-06\n",
      "[2000]\tvalid_set's l2: 2.18201e-06\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.36s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: LightGBM ... Training model for up to 297.38s of the 297.38s of remaining time.\n",
      "\tTraining LightGBM with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE ... Training model for up to 296.83s of the 296.83s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t10.79s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: CatBoost ... Training model for up to 285.76s of the 285.76s of remaining time.\n",
      "\tTraining CatBoost with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "\tWarning: CatBoost on GPU is experimental. If you encounter issues, use CPU for training CatBoost instead.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t13.58s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE ... Training model for up to 272.18s of the 272.18s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.8s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI ... Training model for up to 270.1s of the 270.09s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t8.23s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: XGBoost ... Training model for up to 261.84s of the 261.84s of remaining time.\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:28:18] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "/home/aless/progetti/ML-techniques-for-State-Estimation/venv/lib/python3.8/site-packages/xgboost/core.py:160: UserWarning: [13:28:19] WARNING: /workspace/src/common/error_msg.cc:27: The tree method `gpu_hist` is deprecated since 2.0.0. To use GPU training, set the `device` parameter to CUDA instead.\n",
      "\n",
      "    E.g. tree_method = \"hist\", device = \"cuda\"\n",
      "\n",
      "  warnings.warn(smsg, UserWarning)\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t1.37s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: NeuralNetTorch ... Training model for up to 260.44s of the 260.44s of remaining time.\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t5.71s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge ... Training model for up to 254.72s of the 254.72s of remaining time.\n",
      "\tTraining LightGBMLarge with GPU, note that this may negatively impact model quality compared to CPU training.\n",
      "Warning: GPU mode might not be installed for LightGBM, GPU training raised an exception. Falling back to CPU training...Refer to LightGBM GPU documentation: https://github.com/Microsoft/LightGBM/tree/master/python-package#build-gpu-versionOne possible method is:\tpip uninstall lightgbm -y\tpip install lightgbm --install-option=--gpu\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t2.82s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 299.88s of the 251.86s of remaining time.\n",
      "\tEnsemble Weights: {'NeuralNetFastAI': 0.45, 'LightGBMXT': 0.15, 'CatBoost': 0.15, 'LightGBMLarge': 0.1, 'ExtraTreesMSE': 0.05, 'XGBoost': 0.05, 'NeuralNetTorch': 0.05}\n",
      "\t-0.0\t = Validation score   (-mean_squared_error)\n",
      "\t0.04s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 48.2s ... Best model: \"WeightedEnsemble_L2\"\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"./autogluon_models_net18v1_300/model_17\")\n"
     ]
    }
   ],
   "source": [
    "alt_x = np.load('./simulations_net18/net_18_v1/measured_data_x_alt.npy')\n",
    "alt_y = np.load('./simulations_net18/net_18_v1/data_y_alt.npy')\n",
    "data_x = alt_x\n",
    "data_y = alt_y\n",
    "\n",
    "split_train = int(0.8 * data_x.shape[0])\n",
    "train_x = data_x[:split_train, :]\n",
    "train_y = data_y[:split_train, :]\n",
    "\n",
    "train_x, val_x, train_y, val_y = train_test_split(train_x, train_y, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_x, train_y, test_size=0.3, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "num_input = 53\n",
    "num_output = 18\n",
    "\n",
    "in_columns = [str(i) for i in range(num_input)]\n",
    "out_columns = [str(i) for i in range(num_input, num_input + num_output)]\n",
    "\n",
    "train_models = True\n",
    "if train_models:\n",
    "    for i in range(num_output):\n",
    "        print(f'Training for output #{i}')\n",
    "        train = np.hstack((train_x, train_y[:, i].reshape(-1, 1)))\n",
    "        columns_names = in_columns + [out_columns[i]]\n",
    "        train = pd.DataFrame(train, columns=columns_names)\n",
    "        \n",
    "        x = in_columns\n",
    "        y = out_columns[i]\n",
    "        label = y\n",
    "        model_path = f'./autogluon_models_net18v1_300/model_{i}'\n",
    "        \n",
    "        predictor = TabularPredictor(label=label, problem_type='regression', eval_metric='mean_squared_error', path=model_path).fit(train, presets='medium_quality', num_gpus=1, time_limit=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdd7648c-922e-4ed7-b623-d3699ff0c649",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'./autogluon_models_net18v1_600/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor.load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0790af0f-f08c-49b2-b9af-dd0bb1ae4f77",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 1, CASE 2 VALIDATION\n",
      "SCENARIO 1, CASE 3 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 2 VALIDATION\n",
      "SCENARIO 2, CASE 3 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 2 VALIDATION\n",
      "SCENARIO 3, CASE 3 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 2 VALIDATION\n",
      "SCENARIO 4, CASE 3 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 2 VALIDATION\n",
      "SCENARIO 5, CASE 3 VALIDATION\n",
      "['rmse: 0.007527988405522801 mse: 5.667060943368573e-05', 'rmse: 0.005218273927207544 mse: 2.7230382779374043e-05', 'rmse: 0.003344385723781033 mse: 1.1184915869430385e-05', 'rmse: 0.0038627706897008075 mse: 1.4920997401211652e-05', 'rmse: 0.0012778463922783756 mse: 1.6328914022588603e-06']\n"
     ]
    }
   ],
   "source": [
    "from net18.scenarios2 import get_data_by_scenario_and_case, report_preds_on_validation_files\n",
    "np.set_printoptions(suppress = True, formatter = {'float_kind':'{:0.6f}'.format})\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    for case in range(1, 4):\n",
    "        print(f'SCENARIO {scenario}, CASE {case} VALIDATION')\n",
    "        s1_c1_data = get_data_by_scenario_and_case(scenario, case)\n",
    "        x = s1_c1_data[0]\n",
    "        x_hat = s1_c1_data[1]\n",
    "        y_all = s1_c1_data[2]\n",
    "        y_hat_all = s1_c1_data[3]\n",
    "        \n",
    "        estim = []\n",
    "        for i in range(num_output):\n",
    "            columns_names = in_columns + [out_columns[i]]\n",
    "            x = in_columns\n",
    "            y = out_columns[i]\n",
    "            \n",
    "            predictor = models[i]\n",
    "            test_x = x_hat\n",
    "            test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "            test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "            \n",
    "            \n",
    "            preds = predictor.predict(test)\n",
    "            \n",
    "            estim.append(preds[0])\n",
    "            \n",
    "        pred = np.asarray(estim)\n",
    "        # report_preds_on_validation_files(pred, 8, 'autogloun', scenario, case=case)\n",
    "        if case == 1:\n",
    "            std_results.append(f'rmse: {np.sqrt(np.mean(np.square(y_all - pred)))} mse: {np.mean(np.square(y_all - pred))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5901653-cddc-4a21-86a0-71bdf5d5eaa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 0 loaded\n",
      "Model 1 loaded\n",
      "Model 2 loaded\n",
      "Model 3 loaded\n",
      "Model 4 loaded\n",
      "Model 5 loaded\n",
      "Model 6 loaded\n",
      "Model 7 loaded\n",
      "Model 8 loaded\n",
      "Model 9 loaded\n",
      "Model 10 loaded\n",
      "Model 11 loaded\n",
      "Model 12 loaded\n",
      "Model 13 loaded\n",
      "Model 14 loaded\n",
      "Model 15 loaded\n",
      "Model 16 loaded\n",
      "Model 17 loaded\n"
     ]
    }
   ],
   "source": [
    "models = []\n",
    "for i in range(num_output):\n",
    "    model_path = f'./autogluon_models_net18v1_300/model_{i}/'\n",
    "    files = os.listdir(model_path)\n",
    "    #model_filename = [f for f in files if os.path.isfile(os.path.join(model_path, f))][0]\n",
    "    label = out_columns[i]\n",
    "    \n",
    "    predictor = TabularPredictor.load(f'{model_path}')\n",
    "    models.append(predictor)\n",
    "    print(f'Model {i} loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "15c9796d-887a-4379-80f1-c2d5e9d441af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCENARIO 1, CASE 1 VALIDATION\n",
      "SCENARIO 1, CASE 2 VALIDATION\n",
      "SCENARIO 1, CASE 3 VALIDATION\n",
      "SCENARIO 2, CASE 1 VALIDATION\n",
      "SCENARIO 2, CASE 2 VALIDATION\n",
      "SCENARIO 2, CASE 3 VALIDATION\n",
      "SCENARIO 3, CASE 1 VALIDATION\n",
      "SCENARIO 3, CASE 2 VALIDATION\n",
      "SCENARIO 3, CASE 3 VALIDATION\n",
      "SCENARIO 4, CASE 1 VALIDATION\n",
      "SCENARIO 4, CASE 2 VALIDATION\n",
      "SCENARIO 4, CASE 3 VALIDATION\n",
      "SCENARIO 5, CASE 1 VALIDATION\n",
      "SCENARIO 5, CASE 2 VALIDATION\n",
      "SCENARIO 5, CASE 3 VALIDATION\n",
      "['rmse: 0.007527988405522801 mse: 5.667060943368573e-05', 'rmse: 0.005218273927207544 mse: 2.7230382779374043e-05', 'rmse: 0.003344385723781033 mse: 1.1184915869430385e-05', 'rmse: 0.0038627706897008075 mse: 1.4920997401211652e-05', 'rmse: 0.0012778463922783756 mse: 1.6328914022588603e-06']\n"
     ]
    }
   ],
   "source": [
    "from net18.scenarios2 import get_data_by_scenario_and_case, report_preds_on_validation_files\n",
    "np.set_printoptions(suppress = True, formatter = {'float_kind':'{:0.6f}'.format})\n",
    "\n",
    "std_results = []\n",
    "for scenario in range(1, 6):\n",
    "    for case in range(1, 4):\n",
    "        print(f'SCENARIO {scenario}, CASE {case} VALIDATION')\n",
    "        s1_c1_data = get_data_by_scenario_and_case(scenario, case)\n",
    "        x = s1_c1_data[0]\n",
    "        x_hat = s1_c1_data[1]\n",
    "        y_all = s1_c1_data[2]\n",
    "        y_hat_all = s1_c1_data[3]\n",
    "        \n",
    "        estim = []\n",
    "        for i in range(num_output):\n",
    "            columns_names = in_columns + [out_columns[i]]\n",
    "            x = in_columns\n",
    "            y = out_columns[i]\n",
    "            \n",
    "            predictor = models[i]\n",
    "            test_x = x_hat\n",
    "            test_y = np.asarray(y_all[0][i]).reshape(-1, 1)\n",
    "            test = pd.DataFrame(np.hstack((test_x, test_y)), columns=columns_names)\n",
    "            \n",
    "            \n",
    "            preds = predictor.predict(test)\n",
    "            \n",
    "            estim.append(preds[0])\n",
    "            \n",
    "        pred = np.asarray(estim)\n",
    "        # report_preds_on_validation_files(pred, 8, 'autogloun', scenario, case=case)\n",
    "        if case == 1:\n",
    "            std_results.append(f'rmse: {np.sqrt(np.mean(np.square(y_all - pred)))} mse: {np.mean(np.square(y_all - pred))}')\n",
    "print(std_results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e6dfce5-5eb4-425f-b2c9-909f535aa2f5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
